= Manual de operaciones

== Generación de imágenes personalizadas

Para la generación de imágenes personalizadas en los diferentes proveedores _cloud_ se ha creado una documentación de referencia para cada uno de ellos que se puede encontrar en los siguientes enlaces:

* xref:operations-manual:image-builder/aws-image-builder.adoc[AWS]
* xref:operations-manual:image-builder/azure-image-builder.adoc[Azure]
* xref:operations-manual:image-builder/gcp-image-builder.adoc[GCP]

== Obtención del _kubeconfig_

Para comunicarse con el _API Server_ del _cluster_ creado es necesario el fichero _kubeconfig_, que se obtendrá de forma diferente según el proveedor _cloud_ utilizado y la gestión del _control-plane_ del _cluster_.

* Para EKS, se obtendrá de la forma indicada por AWS:
+
[source,bash]
----
aws eks update-kubeconfig --region eu-west-1 --name <cluster_name> --kubeconfig ./<cluster_name>.kubeconfig
----

* Para GCP, Azure no gestionado y AKS, al finalizar del aprovisionamiento, el _kubeconfig_ se deja en el directorio de ejecución (_workspace_):
+
[source,bash]
----
ls ./.kube/config
./.kube/config
----
+
A su vez, podrá utilizarse el alias "kw" desde el contenedor local para interactuar con el _cluster worker_ (en EKS, el _token_ utilizado sólo dura 10 minutos):
+
[source,bash]
----
root@example-azure-control-plane:/# kw get nodes
NAME                                STATUS   ROLES           AGE   VERSION
example-azure-control-plane-6kp94   Ready    control-plane   60m   v1.26.8
example-azure-control-plane-fgkcc   Ready    control-plane   63m   v1.26.8
...
----

== Autenticación en EKS

Si bien no forma parte de la operativa de _Stratio KEOS_, es importante resaltar la forma de permitir la https://docs.aws.amazon.com/es_es/eks/latest/userguide/add-user-role.html[autenticación de otros usuarios en un _cluster_ de EKS] (el usuario creador del _cluster_ está autenticado por defecto).

Para dar permisos de _kubernetes-admin_ en el _cluster_, se agregará el ARN del usuario en el _ConfigMap_ indicado a continuación.

[source,bash]
----
$ kubectl -n kube-system edit cm aws-auth
..
data:
  mapUsers: |
    - groups:
      - system:masters
      userarn: <user_arn>
      username: kubernetes-admin
----

== Operación de la infraestructura

image::controllers.png[]

_Stratio KEOS_ permite realizar múltiples operaciones avanzadas interactuando con el _Stratio Cluster Operator_ (_infrastructure as code_ o IaC), quien en su ciclo de reconciliación interactúa a su vez con los distintos proveedores para realizar las operaciones solicitadas.

=== _Self-healing_

image::self-healing.png[]

La capacidad de _self-healing_ del _cluster_ se gestiona por el objeto _MachineHealthCheck_:

[source,bash]
----
$ kubectl -n cluster-example get mhc -o yaml
...
  spec:
    clusterName: example
    maxUnhealthy: 100%
    nodeStartupTimeout: 5m0s
    selector:
      matchLabels:
        keos.stratio.com/machine-role: example-worker-node
    unhealthyConditions:
    - status: Unknown
      timeout: 1m0s
      type: Ready
    - status: "False"
      timeout: 1m0s
      type: Ready
...
----

NOTE: Los proveedores no gestionados tendrán un _MachineHealthCheck_ para el _control-plane_ y otro para los nodos _worker_, mientras que los gestionados (EKS, AKS) sólo tendrán el segundo.

==== Prueba de tolerancia a fallos en un nodo

En caso de fallo en un nodo, este será detectado por un _controller_ y se procederá al reemplazo del mismo, eliminándolo y volviendo a crear otro del mismo grupo, lo que asegura las mismas características.

Para simular un fallo en una máquina virtual, se eliminará desde la consola web del proveedor de _cloud_.

La recuperación del nodo comprende las siguientes fases y tiempos estimados (pudiendo variar según el proveedor y el _flavour_):

[source,bash]
----
. Terminate VM from console: 0s
. New VM is Provisioning: 50s
. Old Machine is Deleted & the new one is Provisioned: 1m5s
. New Machine is Running & new k8s node is NotReady: 1m 50s
. New k8s node is Ready: 2m
----

=== Escalado estático

Aunque se desaconseja el escalado manual de un grupo de nodos existente, se presentan estas operaciones para casos sin autoescalado o nuevos grupos de nodos.

==== Escalar un grupo de _workers_

image::escalado-manual.png[]

Para escalar manualmente un grupo de _workers_ se usa el objeto _KeosCluster_:

[source,bash]
----
kubectl -n cluster-example-eks edit keoscluster
----

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - name: eks-prod-xlarge
      quantity: 9
      ...
----

Verifica el cambio consultando el estado del objeto _KeosCluster_:

[source,bash]
----
kubectl -n cluster-<cluster_name> get keoscluster <cluster_name> --subresource=status
----

===== AKS

En el caso de este proveedor, si están definidos los parámetros `min_size` y `max_size` en el objeto _KeosCluster_ no se realiza ninguna acción.

NOTE: Los grupos de nodos del objeto _KeosCluster_ se corresponden en Azure a _Node pools_ dentro de AKS y sus correspondientes _VM Scale Sets_.

El escalado manual de un grupo de nodos en AKS con el autoescalado configurado se deberá hacer desde el portal de Azure en:

'VM Scale set' -> '<scale_set_name>' -> 'Scalling' -> '<instance_number>'

o bien desde:

'Kubernetes services' -> '<aks_name>' -> 'Node pools' -> '<nodepool_name>' -> 'Scale node pool' -> 'Manual' -> '<node_count>'

Las nuevas instancias se pueden ver en 'VM Scale set' -> 'Instances'. Este cambio no se reflejará en el parámetro `quantity` del grupo de nodos del objeto _KeosCluster_.

Los tiempos estimados de este proceso son los siguientes:

[source,bash]
----
Scale VM Scale set: 0s
New K8s node is NotReady: 1m
New K8s node is Ready: 1m 13s
The MachinePool Scaling: 1m 29s
The MachinePool is updated: 1m 33s
----

==== Crear un nuevo grupo de _workers_

Para crear un nuevo grupo de nodos basta con crear un nuevo elemento al _array_ _worker++_++nodes_ del objeto _KeosCluster_:

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - ...
    - name: eks-prod-xlarge
      quantity: 6
      max_size: 18
      min_size: 6
      size: m6i.xlarge
      labels:
        disktype: standard
      root_volume:
        size: 50
        type: gp3
        encrypted: true
      ssh_key: stg-key
----

Nuevamente, verifica el cambio consultando el estado del objeto _KeosCluster_:

[source,bash]
----
kubectl -n cluster-<cluster_name> get keoscluster <cluster_name> --subresource=status
----

==== Escalado vertical

CAUTION: *AKS no soporta escalado vertical* de los grupos de nodos. Para este proveedor, se deberá crear un grupo nuevo y eliminar el anterior como lo indica la https://learn.microsoft.com/en-us/azure/aks/resize-node-pool[documentación oficial] ^[English]^.

El escalado vertical de un grupo de nodos se realiza modificando el tipo de instancia en el objeto _KeosCluster_ correspondiente al grupo.

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - name: eks-prod-xlarge
      size: m6i.2xlarge
      ...
----

=== Autoescalado

image::autoescalado.png[]

Para el autoescalado de nodos se utiliza _cluster-autoscaler_, quien detectará _pods_ pendientes de ejecutar por falta de recursos y escalará el grupo de nodos que considere según los filtros de los despliegues.

Esta operación se realiza en el _API Server_, siendo los _controllers_ los encargados de crear las máquinas virtuales en el proveedor de _cloud_ y agregarlas al _cluster_ como nodos _worker_ de Kubernetes.

Dado que el autoescalado está basado en el _cluster-autoscaler_, se añadirá el mínimo y máximo en el grupo de nodos en el objeto _KeosCluster_:

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - name: eks-prod-xlarge
      min_size: 6
      max_size: 21
      ...
----

===== AKS

En este proveedor el autoescalado se gestiona desde los _VM Scale sets_ de Azure y no con el _cluster-autoscaler_.

Durante el aprovisionamiento, en el momento de crear los grupos de nodos se instanciarán los _Node pools_ en AKS y sus respectivos _VM Scale Sets_. Si los grupos de nodos definidos tienen un rango de autoescalado, estos se trasladarán a los _Node pools_ creados.

Para verlos en el portal de Azure, se deberá consultar:

'Kubernetes services' -> 'aks_name>' -> 'Node pools' -> '<nodepool_name>' -> 'Scale node pool' -> 'Autoscale'.

==== Prueba

Para probar el autoescalado, se puede crear un _Deployment_ con suficientes réplicas de modo que no se puedan ejecutar en los nodos actuales:

[source,bash]
----
kubectl create deploy test --replicas 1500 --image nginx:alpine
----

Al terminar la prueba, se elimina el _Deployment_:

[source,bash]
----
kubectl --kubeconfig demo-eks.kubeconfig delete deploy test
----

==== _Logs_

Los _logs_ del _cluster-autoscaler_ se pueden ver desde su _Deployment_:

[source,bash]
----
kubectl -n kube-system logs -f -l app.kubernetes.io/name=clusterapi-cluster-autoscaler
----

=== Actualización de versión de _Stratio Cloud Provisioner_ a 0.5

==== Antes de empezar

A diferencia de otras versiones del _script_ de _upgrade_, todas las etapas hasta completar la actualización se hacen de manera automatizada y sin interacción humana excepto para confirmaciones y solicitudes de información de versiones. Durante la actualización, es importante tener en cuenta que se realizará un cambio consecutivo de la versión de Kubernetes, siguiendo el procedimiento automatizado que se detalla a continuación:

- Realizar un proceso de _backup_ de los objetos que administran la infraestructura.
- Actualizar y/o añadir servicios dentro del _cluster_ de Kubernetes.
- Actualizar la versión de kubernetes de 1.26.X a 1.27.X.
- Actualizar la versión de kubernetes de 1.27.X a 1.28.X.
- Restaurar, si es necesario, los servicios del proceso de _backup_ realizado previamente.

En cuanto a la actualización de las versiones de Kubernetes, esta se realizará en dos etapas dentro del mismo proceso atómico: primero, el _control-plane_, y una vez que esté en la nueva versión, los nodos _worker_, iterando por cada grupo de nodos y actualizándolos uno a uno de manera secuencial.

CAUTION: La actualización de la versión de kubernetes de los nodos en los clústeres donde no se haya especificado la imagen puede implicar una actualización del sistema operativo.

==== Prerrequisitos

- Los siguientes binarios deberán estar disponibles en la máquina bastión:

  - python3
  - ansible-vault (pip)
  - clusterctl
  - helm
  - kubectl
  - jq
  - aws (opcional)
  - az (opcional)
  - gcloud (opcional)

- Se deberán asegurar los permisos necesarios en el directorio _backup_ de la máquina bastión para que el usuario que ejecuta el _script_ pueda escribir en él (se crea el directorio _./backup/upgrade/_).

- Asegurar la compatibilidad de la versión de kubernetes para los diferentes _providers_, con especial atención para clústeres cuyas grupos de nodos no tienen definida una imagen _custom_:
** EKS:
+
[source,bash]
----
aws eks describe-addon-versions | jq -r ".addons[] | .addonVersions[] | .compatibilities[] | .clusterVersion" | sort -nr | uniq | head -4
----

* AWS VMs:
+
[source,bash]
----
aws ec2 describe-images --filters "Name=name,Values=capa-ami-ubuntu-18.04-*" --query 'Images[*].{ID:ImageId,Name:Name}' --output table
----

* AKS
+
[source,bash]
----
az aks get-versions --location <region> --output table
----
* Azure VMs
+
[source,bash]
----
az vm image list --publisher cncf-upstream --offer "capi" --sku ubuntu-2204  --all -o table
----

* GCP VMs. No aplica ya que es obligatorio especificar una imagen _custom_.

* En entornos productivos y especialmente en proveedores no gestionados, deberá extremarse la precaución. En particular, antes de actualizar, se recomienda hacer un _backup_ de los objetos que gestionan la infraestructura y de los servicios considerados críticos.

==== Ejecución

Durante la ejecución del _script_ se requerirá al usuario confirmar la continuación del proceso en diversas etapas y proporcionar información relevante, como la versión de Kubernetes que se desea actualizar.

Se debe ejecutar el _script_ _upgrade-provisioner++_++.py_ cuya ayuda se puede consultar con el siguiente comando:

[source,bash]
----
python3 upgrade-provisioner.py -h
----

Ejemplo básico:

[source,bash]
----
python3 upgrade-provisioner.py -p <vault_pass>
----

Ejemplo de salida de la ejecución:

[source,bash]
----
[INFO] Using kubeconfig: /tmp/kubeconfig
[INFO] Cluster name: esierra-dev-vms
Press ENTER to continue upgrading the cluster or any other key to abort: 
[INFO] Verifying upgrade process
[INFO] Backing up files into directory ./backup/upgrade/20240611-132257
[INFO] Backing up CAPX files: OK
[INFO] Backing up capsule files: OK
[INFO] Preparing capsule-mutating-webhook-configuration for the upgrade process: OK
[INFO] Preparing capsule-validating-webhook-configuration for the upgrade process: OK
[INFO] Applying new ClusterConfig CRD: OK
[INFO] Upgrading Cluster Operator 0.3.0: OK
[INFO] Restoring capsule-mutating-webhook-configuration: OK
[INFO] Restoring capsule-validating-webhook-configuration: OK
Please provide the Kubernetes version to which you want to upgrade: 1.27.11
Are you sure you want to upgrade to version 1.27.11? (yes/no): y
[INFO] Initiating upgrade to kubernetes to version 1.27.11
[INFO] Scaling down cluster autoscaler replicas: OK
[INFO] Applying temporal allow control plane GlobalNetworkPolicy: OK
Please provide the image ID associated with the Kubernetes version: 1.27.11 for control-plane: ami-09d1a38098ccd9d16
Are you sure you want to use node image: ami-09d1a38098ccd9d16 for control-plane? (yes/no): y 
Please provide the image ID associated with the Kubernetes version: 1.27.11 for worker node: worker1: ami-09d1a38098ccd9d16
Are you sure you want to use node image: ami-09d1a38098ccd9d16 for worker node: worker1? (yes/no): y
Please provide the image ID associated with the Kubernetes version: 1.27.11 for worker node: minmax0: ami-09d1a38098ccd9d16
Are you sure you want to use node image: ami-09d1a38098ccd9d16 for worker node: minmax0? (yes/no): y
[INFO] node_image is not defined in worker node: noimage
[INFO] Waiting for the Kubernetes version upgrade - control plane: OK
[INFO] Waiting for the Kubernetes version upgrade - worker nodes: OK
[INFO] Restoring allow control plane GlobalNetworkPolicy: OK
[INFO] Scaling up cluster autoscaler replicas: OK
Please provide the Kubernetes version to which you want to upgrade: 1.28.7
Are you sure you want to upgrade to version 1.28.7? (yes/no): y
[INFO] Initiating upgrade to kubernetes to version 1.28.7
[INFO] Scaling down cluster autoscaler replicas: OK
[INFO] Applying temporal allow control plane GlobalNetworkPolicy: OK
Please provide the image ID associated with the Kubernetes version: 1.28.7 for control-plane: ami-0a226d9d637560c4b
Are you sure you want to use node image: ami-0a226d9d637560c4b for control-plane? (yes/no): yes
Please provide the image ID associated with the Kubernetes version: 1.28.7 for worker node: worker1: ami-0a226d9d637560c4b
Are you sure you want to use node image: ami-0a226d9d637560c4b for worker node: worker1? (yes/no): yes
Please provide the image ID associated with the Kubernetes version: 1.28.7 for worker node: minmax0: ami-0a226d9d637560c4b
Are you sure you want to use node image: ami-0a226d9d637560c4b for worker node: minmax0? (yes/no): yes
[INFO] node_image is not defined in worker node: noimage
[INFO] Waiting for the Kubernetes version upgrade - control plane: OK
[INFO] Waiting for the Kubernetes version upgrade - worker nodes: OK
[INFO] Restoring allow control plane GlobalNetworkPolicy: OK
[INFO] Scaling up cluster autoscaler replicas: OK
[INFO] Upgrade process finished successfully in 117 minutes and 14.68 seconds
----

En caso de fallo, al tratarse de un _script_ indempotente, se podrá ejecutar tantas veces como se desea y se obtenga el mensaje de finalización de actualización satisfactoria.

==== Verificación post-actualización

===== etcd

Una forma de asegurar que el etcd está correcto después de actualizar un _control-plane_ no gestionado es abrir una terminal en cualquier _pod_ de etcd, ver el estado del _cluster_ y comparar las IP de los miembros registrados con las de los nodos del _control-plane_.

[source,bash]
----
kubectl -n kube-system exec -ti etcd-<control-plane-node> sh

alias e="etcdctl --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt "
e endpoint status
e endpoint status -w table --cluster
e member list
e member remove <member-id>
----

===== cluster-autoscaler

Se ha de comprobar que el _deployment_ de cluster-autoscaler está configurado con su número original de réplicas, esto es, igual a 2.

[source,bash]
----
kubectl get deploy cluster-autoscaler-clusterapi-cluster-autoscaler -n kube-system -ojsonpath='{.status.replicas}'
----

=== Eliminación del _cluster_

[NOTE]
.Consideraciones previas
====
Antes de eliminar los recursos del proveedor _cloud_ generados por _Stratio Cloud Provisioner_ se deberán eliminar aquellos creados por _keos-installer_ o cualquier automatismo externo (por ejemplo, los _Services_ de tipo _LoadBalancer_).

Además, deberás tener en cuenta que el proceso requiere del binario del _clusterctl_ en la máquina bastión (cualquier ordenador con acceso al _API Server_) en la que se va a ejecutar.
====

Ejecuta los siguientes pasos para llevar a cabo la eliminación del _cluster_:

. Crea un _cluster_ local indicando que no se genere ningún objeto en el proveedor _cloud_.
+
[source,bash]
----
[bastion]$ sudo ./bin/cloud-provisioner create cluster --name <cluster_name> --descriptor cluster.yaml --vault-password <my-passphrase> --avoid-creation
----

. Pausa el _controller_ del _Stratio Cluster Operator_:
+
[source,bash]
----
[bastion]$ kubectl --kubeconfig $KUBECONFIG -n kube-system scale deployment keoscluster-controller-manager --replicas 0
----

. Mueve la gestión del _cluster_ _worker_ al _cluster_ local utilizando el _kubeconfig_ correspondiente (para los _control-planes_ gestionados, se necesitará el _kubeconfig_ del proveedor). Para asegurar este paso, se buscará el siguiente texto en la salida del comando: "Moving Cluster API objects Clusters=1".
+
[source,bash]
----
[bastion]$ sudo clusterctl --kubeconfig $KUBECONFIG move -n cluster-<cluster_name> --to-kubeconfig /root/.kube/config
----

. Accede al _cluster_ local y elimina el _cluster_ _worker_.
+
[source,bash]
----
[bastion]$ sudo docker exec -ti <cluster_name>-control-plane bash
root@<cluster_name>-control-plane:/# kubectl -n cluster-<cluster_name> delete cl --all
cluster.cluster.x-k8s.io "<cluster_name>" deleted
root@<cluster_name>-control-plane:/#
----

. Finalmente, elimina el _cluster_ local.
+
[source,bash]
----
[bastion]$ sudo ./bin/cloud-provisioner delete cluster --name <cluster_name>
----

== Instalación _offline_

Para saber cómo llevar a cabo una instalación en la que las imágenes de los _workloads_ del _cluster_ provengan de repositorios accesibles desde entornos sin acceso a internet, consulta el xref:operations-manual:offline-installation.adoc[Manual de instalación _offline_].
