= Manual de operaciones

== Obtención del _kubeconfig_

Para comunicarse con el _API Server_ del _cluster_ creado es necesario el fichero _kubeconfig_, que se obtendrá de forma diferente según el proveedor _cloud_ utilizado y la gestión del _control-plane_ del _cluster_.

* Para EKS, se obtendrá de la forma indicada por AWS:
+
[source,bash]
----
aws eks update-kubeconfig --region eu-west-1 --name <cluster_name> --kubeconfig ./<cluster_name>.kubeconfig
----

* Para GKE, se obtendrá de la forma indicada por GCP:
+
[source,bash]
----
gcloud container clusters get-credentials <cluster_name> --region <region> --project <project>
----

Este comando generará el fichero con el _kubeconfig_ donde indique la variable de entorno `KUBECONFIG`. Por defecto, será en `$HOME/.kube/config`.

* Para Azure no gestionado, al finalizar del aprovisionamiento el _kubeconfig_ se deja en el directorio de ejecución (_workspace_):
+
[source,bash]
----
ls ./.kube/config
./.kube/config
----
+
A su vez, podrá utilizarse el alias "kw" desde el contenedor local para interactuar con el _cluster worker_ (en EKS, el _token_ utilizado sólo dura 10 minutos):
+
[source,bash]
----
root@example-azure-control-plane:/# kw get nodes
NAME                                STATUS   ROLES           AGE   VERSION
example-azure-control-plane-6kp94   Ready    control-plane   60m   v1.26.8
example-azure-control-plane-fgkcc   Ready    control-plane   63m   v1.26.8
...
----

== Autenticación en EKS

Si bien no forma parte de la operativa de _Stratio KEOS_, es importante resaltar la forma de permitir la https://docs.aws.amazon.com/es_es/eks/latest/userguide/add-user-role.html[autenticación de otros usuarios en un _cluster_ de EKS] (el usuario creador del _cluster_ está autenticado por defecto).

Para dar permisos de _kubernetes-admin_ en el _cluster_, se agregará el ARN del usuario en el _ConfigMap_ indicado a continuación.

[source,bash]
----
$ kubectl -n kube-system edit cm aws-auth
..
data:
  mapUsers: |
    - groups:
      - system:masters
      userarn: <user_arn>
      username: kubernetes-admin
----

== Activar autorización por _assume role_ en AWS para un _cluster_ EKS

Este procedimiento describe cómo habilitar el método de autorización basado en _assume role_ en AWS para un _cluster_ de EKS una vez que ha sido creado.

=== Prerrequisitos

Antes de comenzar, asegúrate de cumplir lo siguiente:

* Tener creado un rol de IAM con los permisos necesarios (por ejemplo: _stratio-assume-role_).
** Tipo de role: _AWS Account_.
** Permisos: los mismos que los descritos en los prerrequisitos de EKS.
* Configurar la _Trust Relationship_ del rol para permitir que el usuario que desplegó el _cluster_ pueda asumirlo.
+
Ejemplo de _Trust Relationship_:
+
[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:user/stratio-user"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
----

* Asignar al usuario una política que incluya la acción `sts:AssumeRole` sobre el rol correspondiente.
+
Ejemplo de política:
+
[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "sts:AssumeRole",
      "Resource": "arn:aws:iam::123456789012:role/stratio-assume-role"
    }
  ]
}
----

NOTE: La versión mínima de _cluster-operator_ compatible con este método es la 0.5.2.

=== Configurar _assume role_ en el _cluster_ EKS

NOTE: Sustituye la cuenta, nombre del _cluster_ `eks-cl01` y el _namespace_ `cluster-eks-cl01` por los de tu entorno.

. Crea el objeto `AWSClusterRoleIdentity`.
+
[source,yaml]
-----
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSClusterRoleIdentity
metadata:
  name: eks-cl01-role-identity
spec:
  roleARN: arn:aws:iam::123456789012:role/stratio-assume-role
  sessionName: eks-cl01-role-identity-session
  durationSeconds: 3600
  sourceIdentityRef:
    kind: AWSClusterControllerIdentity
    name: default
  allowedNamespaces:
    list:
    - cluster-eks-cl01
-----

. Asocia la identidad `AWSClusterRoleIdentity` al `AWSManagedControlPlane`.
+
[source,bash]
-----
kubectl patch awsmanagedcontrolplane eks-cl01-control-plane \
  -n cluster-eks-cl01 \
  --type='merge' \
  -p '{"spec":{"identityRef":{"kind":"AWSClusterRoleIdentity","name":"eks-cl01-role-identity"}}}'
-----

. Reinicia el _deployment_ de `capa-controller-manager` para que los cambios surtan efecto.
+
[source,bash]
-----
kubectl rollout restart deployment capa-controller-manager -n capa-system
-----

. Configura el `aws-auth` con el `iam_role`.
+
[source,yaml]
-----
kubectl patch configmap aws-auth -n kube-system --type merge -p '
data:
  mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: arn:aws:iam::123456789012:role/nodes.cluster-api-provider-aws.sigs.k8s.io
      username: system:node:{{EC2PrivateDNSName}}
    - groups:
      - capa-manager
      rolearn: arn:aws:iam::963353512345678901211234:role/stratio-assume-role
      username: stratio-assume-role
'
-----

. Crea el `ClusterRoleBinding` para el grupo `capa-manager`.
+
[source,yaml]
-----
kubectl apply -f - <<EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: capa-manager-access
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: capa-manager-role
subjects:
- kind: Group
  name: capa-manager
  apiGroup: rbac.authorization.k8s.io
EOF
-----

. Añade permisos al `ClusterRole` del grupo `capa-manager`.
+
[source,bash]
-----
kubectl patch clusterrole capa-manager-role \
  --type='json' \
  -p='[
    {"op": "add", "path": "/rules/-", "value": {"apiGroups":["apps"],"resources":["daemonsets"],"verbs":["get","list","watch","update"]}},
    {"op": "add", "path": "/rules/-", "value": {"apiGroups":[""],"resources":["pods"],"verbs":["get","list","watch"]}},
    {"op": "add", "path": "/rules/-", "value": {"apiGroups":[""],"resources":["nodes"],"verbs":["get","list","watch","patch"]}},
    {"op": "add", "path": "/rules/-", "value": {"apiGroups":[""],"resources":["pods/eviction"],"verbs":["create"]}}
  ]'
-----

. Actualiza el _cluster-operator_.
.. Actualiza el _helmrelease_ asociado al _cluster-operator_ a la versión 0.5.2 o superior.
.. Actualiza el _ConfigMap_ correspondiente a esa versión.
.. Parchea el secreto _keoscluster-settings_ para añadir el `role_arn`.
+
[source,bash]
------
kubectl -n kube-system patch secret keoscluster-settings \
  --type=json \
  -p='[{"op":"replace","path":"/data/credentials","value":"'$(kubectl -n kube-system get secret keoscluster-settings -o jsonpath="{.data.credentials}" | base64 -d | awk 'BEGIN{ORS="\n"} {print} END{print "role_arn = arn:aws:iam::123456789012:role/stratio-assume-role"}' | base64 -w0)'"}]'
------

. Verifica la configuración y los permisos.
+
[source,bash]
-----
# Revisar los logs de capi/capa
kubectl logs -f -n capa-system deployment/capa-controller-manager (o el nombre del pod)
kubectl logs -f -n capi-system deployment/capi-controller-manager (o el nombre del pod)

# Revisar los logs del _cluster-operator_
kubectl logs -f -n capa-system deployment/keoscluster-controller-manager (o el nombre del pod)

# Comprobar estado y configuración
kubectl get awsclusterroleidentity
kubectl get awsmanagedcontrolplane -n cluster-eks-cl01
kubectl get configmap aws-auth -n kube-system -o yaml

# Verificar permisos del rol
kubectl auth can-i get nodes --as=stratio-assume-role --as-group=capa-manager
kubectl auth can-i list nodes --as=stratio-assume-role --as-group=capa-manager
kubectl auth can-i list pods --as=stratio-assume-role --as-group=capa-manager
kubectl auth can-i update daemonsets --as=stratio-assume-role --as-group=capa-manager
-----

=== Operaciones del _cluster_ con _assume role_

Una vez activado, también puedes gestionar el _cluster_ utilizando _assume role_ desde línea de comandos siguiendo estos pasos:

. *Verifica AWS CLI*: asegúrate de tener la última versión instalada con este comando.
+
[source,bash]
----
aws --version
----

. *Exporta el perfil de AWS*.
+
[source,bash]
----
export AWS_PROFILE=<nombre-del-profile>
----

. *Asume el rol y guarda credenciales*.
+
[source,bash]
----
aws sts assume-role \
  --role-arn arn:aws:iam::<accountID>:role/<role-name> \
  --role-session-name eks-session > creds.json
----

. *Exporta las credenciales temporales*.
+
[source,bash]
----
export AWS_ACCESS_KEY_ID=$(jq -r '.Credentials.AccessKeyId' creds.json)
export AWS_SECRET_ACCESS_KEY=$(jq -r '.Credentials.SecretAccessKey' creds.json)
export AWS_SESSION_TOKEN=$(jq -r '.Credentials.SessionToken' creds.json)
----

. *Actualiza el _kubeconfig_*.
+
[source,bash]
----
aws eks update-kubeconfig --region <region> --name <nombre-del-cluster>
----

== Operación de la infraestructura

image::controllers.png[]

_Stratio KEOS_ permite realizar múltiples operaciones avanzadas interactuando con el _Stratio Cluster Operator_ (_infrastructure as code_ o IaC), quien en su ciclo de reconciliación interactúa a su vez con los distintos proveedores para realizar las operaciones solicitadas.

=== _Self-healing_

image::self-healing.png[]

La capacidad de _self-healing_ del _cluster_ se gestiona por el objeto _MachineHealthCheck_:

[source,bash]
----
$ kubectl -n cluster-example get mhc -o yaml
...
  spec:
    clusterName: example
    maxUnhealthy: 100%
    nodeStartupTimeout: 5m0s
    selector:
      matchLabels:
        keos.stratio.com/machine-role: example-worker-node
    unhealthyConditions:
    - status: Unknown
      timeout: 1m0s
      type: Ready
    - status: "False"
      timeout: 1m0s
      type: Ready
...
----

NOTE: En Azure no gestionado existirá un _MachineHealthCheck_ para el _control-plane_ y otro para los nodos _worker_, mientras que los gestionados (EKS, GKE) sólo tendrán el segundo.

==== Prueba de tolerancia a fallos en un nodo

En caso de fallo en un nodo, este será detectado por un _controller_ y se procederá al reemplazo del mismo, eliminándolo y volviendo a crear otro del mismo grupo, lo que asegura las mismas características.

Para simular un fallo en una máquina virtual, se eliminará desde la consola web del proveedor de _cloud_.

La recuperación del nodo comprende las siguientes fases y tiempos estimados (pudiendo variar según el proveedor y el _flavour_):

[source,bash]
----
. Terminate VM from console: 0s
. New VM is Provisioning: 50s
. Old Machine is Deleted & the new one is Provisioned: 1m5s
. New Machine is Running & new k8s node is NotReady: 1m 50s
. New k8s node is Ready: 2m
----

=== Escalado estático

Aunque se desaconseja el escalado manual de un grupo de nodos existente, se presentan estas operaciones para casos sin autoescalado o nuevos grupos de nodos.

==== Escalar un grupo de _workers_

image::escalado-manual.png[]

Para escalar manualmente un grupo de _workers_ se usa el objeto _KeosCluster_:

[source,bash]
----
kubectl -n cluster-example-eks edit keoscluster
----

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - name: eks-prod-xlarge
      quantity: 9
      ...
----

Verifica el cambio consultando el estado del objeto _KeosCluster_:

[source,bash]
----
kubectl -n cluster-<cluster_name> get keoscluster <cluster_name> --subresource=status
----

==== Crear un nuevo grupo de _workers_

Para crear un nuevo grupo de nodos basta con crear un nuevo elemento al _array_ _worker++_++nodes_ del objeto _KeosCluster_:

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - ...
    - name: eks-prod-xlarge
      quantity: 6
      max_size: 18
      min_size: 6
      size: m6i.xlarge
      labels:
        disktype: standard
      root_volume:
        size: 50
        type: gp3
        encrypted: true
      ssh_key: stg-key
----

Nuevamente, verifica el cambio consultando el estado del objeto _KeosCluster_:

[source,bash]
----
kubectl -n cluster-<cluster_name> get keoscluster <cluster_name> --subresource=status
----

==== Escalado vertical

El escalado vertical de un grupo de nodos se realiza modificando el tipo de instancia en el objeto _KeosCluster_ correspondiente al grupo.

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - name: eks-prod-xlarge
      size: m6i.2xlarge
      ...
----

=== Autoescalado

image::autoescalado.png[]

Para el autoescalado de nodos se utiliza _cluster-autoscaler_, quien detectará _pods_ pendientes de ejecutar por falta de recursos y escalará el grupo de nodos que considere según los filtros de los despliegues.

Esta operación se realiza en el _API Server_, siendo los _controllers_ los encargados de crear las máquinas virtuales en el proveedor de _cloud_ y agregarlas al _cluster_ como nodos _worker_ de Kubernetes.

Dado que el autoescalado está basado en el _cluster-autoscaler_, se añadirá el mínimo y máximo en el grupo de nodos en el objeto _KeosCluster_:

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - name: eks-prod-xlarge
      min_size: 6
      max_size: 21
      ...
----

==== Prueba

Para probar el autoescalado, se puede crear un _Deployment_ con suficientes réplicas de modo que no se puedan ejecutar en los nodos actuales:

[source,bash]
----
kubectl create deploy test --replicas 1500 --image nginx:alpine
----

Al terminar la prueba, se elimina el _Deployment_:

[source,bash]
----
kubectl --kubeconfig demo-eks.kubeconfig delete deploy test
----

==== _Logs_

Los _logs_ del _cluster-autoscaler_ se pueden ver desde su _Deployment_:

[source,bash]
----
kubectl -n kube-system logs -f -l app.kubernetes.io/name=clusterapi-cluster-autoscaler
----

=== Actualización de Kubernetes

La actualización del _cluster_ a una versión superior de Kubernetes se realizará en dos partes dentro del mismo proceso atómico: primero, el _control-plane_, y una vez que esté en la nueva versión, los nodos _worker_, iterando por cada grupo y actualizándolos uno a uno.

CAUTION: La actualización de la versión de Kubernetes de los nodos en los _clusters_ donde no se haya especificado la imagen puede implicar una actualización del sistema operativo.

image::upgrade-cp.png[]

image::upgrade-w.png[]

[CAUTION]
====
Un _Pod Disruption Budget_ (PDB) mal configurado puede bloquear la eliminación de un _pod_. Esto ocurre si el PDB exige al menos una réplica disponible, pero el recurso solo tiene una desplegada. En ese caso, la réplica no puede eliminarse y el nodo no se puede drenar, lo que puede afectar a las actualizaciones.

Para evitar este problema:

. Asegúrate de que los despliegues tengan más de una réplica si el PDB requiere al menos una disponible.
. Antes de actualizar el _cluster_, revisa esta configuración para evitar bloqueos.
. Si el recurso tiene solo una réplica, puedes eliminar temporalmente el PDB para permitir la actualización.
. Antes de actualizar el _cluster_, revisa los PDB para evitar bloqueos.
. Si un recurso solo tiene una réplica, puedes eliminar temporalmente el PDB para poder actualizar.

En EKS, por ejemplo, es recomendable comprobar si existe el PDB `coredns` en el _namespace_ `kube-system` y eliminarlo antes de actualizar el _cluster_:

[source,bash]
----
kubectl -n kube-system get poddisruptionbudget coredns
kubectl -n kube-system delete poddisruptionbudget coredns
----
====

==== Prerrequisitos

La actualización de versión de un _cluster_ en entornos productivos y especialmente en _flavours_ no gestionados deberá hacerse extremando todas las precauciones. En particular, antes de actualizar se recomienda hacer un _backup_ de los objetos que gestionan la infraestructura con el siguiente comando:

[source,bash]
----
clusterctl --kubeconfig ./kubeconfig/path move -n cluster-<cluster_name> --to-directory ./backup/path/
----

En el caso de un _control-plane_ gestionado, se deberá verificar que la versión deseada de Kubernetes está soportada por el proveedor.

===== EKS

Previo a la actualización de EKS debes asegurarte de que la versión deseada está soportada. Para ello puedes utilizar el siguiente comando:

[source,bash]
----
aws eks describe-addon-versions | jq -r ".addons[] | .addonVersions[] | .compatibilities[] | .clusterVersion" | sort -nr | uniq | head -4
----

===== Azure no gestionado

La _GlobalNetworkPolicy_ creada para el _control-plane_ en la fase de instalación de _Stratio KEOS_ se deberá modificar de modo que *permita toda la red de los nodos momentáneamente* mientras se ejecuta la actualización de versión.

Una vez finalizada, se deberán actualizar las IP internas de los nodos y las de túnel asignadas a dichos nodos:

[source,bash]
----
kubectl get nodes -l node-role.kubernetes.io/control-plane= -ojson | jq -r '.items[].status.addresses[] | select(.type=="InternalIP").address + "\/32"'
----

[source,bash]
----
IPAMHANDLERS=$(kw get ipamhandles -oname | grep control-plane)
for handler in $IPAMHANDLERS; do kw get $handler -o json | jq -r '.spec.block | keys[]' | sed 's/\/.*/\/32/'; done
----

==== Iniciar la actualización

Para iniciar la actualización, una vez satisfechos los prerrequisitos se ejecutará un _patch_ de _spec.k8s++_++version_ en el objeto _KeosCluster_:

[source,bash]
----
kubectl -n cluster-<cluster_name> patch KeosCluster <cluster_name> --type merge -p '{"spec": {"k8s_version": "v1.26.6"}}'
----

NOTE: El _controller_ aprovisiona un nuevo nodo del grupo de _workers_ con la versión actualizada y, una vez que esté _Ready_ en Kubernetes, elimina un nodo con la versión vieja. De esta forma, asegura siempre el número de nodos configurado.

==== Verificación de etcd

Una forma de asegurar que el etcd está correcto después de actualizar un _control-plane_ no gestionado es abrir una terminal en cualquier _pod_ de etcd, ver el estado del _cluster_ y comparar las IP de los miembros registrados con las de los nodos del _control-plane_.

[source,bash]
----
k -n kube-system exec -ti etcd-<control-plane-node> sh

alias e="etcdctl --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt "
e endpoint status
e endpoint status -w table --cluster
e member list
e member remove <member-id>
----

=== Eliminación del _cluster_

[NOTE]
.Consideraciones previas
====
Antes de eliminar los recursos del proveedor _cloud_ generados por _Stratio Cloud Provisioner_ se deberán eliminar aquellos creados por _keos-installer_ o cualquier automatismo externo (por ejemplo, los _Services_ de tipo _LoadBalancer_).

Además, deberás tener en cuenta que el proceso requiere del binario del _clusterctl_ en la máquina bastión (cualquier ordenador con acceso al _API Server_) en la que se va a ejecutar.
====

Ejecuta los siguientes pasos para llevar a cabo la eliminación del _cluster_:

. Crea un _cluster_ local indicando que no se genere ningún objeto en el proveedor _cloud_.
+
[source,bash]
----
[bastion]$ sudo ./bin/cloud-provisioner create cluster --name <cluster_name> --descriptor cluster.yaml --vault-password <my-passphrase> --avoid-creation
----

. Pausa el _controller_ del _Stratio Cluster Operator_:
+
[source,bash]
----
[bastion]$ kubectl --kubeconfig $KUBECONFIG -n kube-system scale deployment keoscluster-controller-manager --replicas 0
----

. Mueve la gestión del _cluster_ _worker_ al _cluster_ local utilizando el _kubeconfig_ correspondiente (para los _control-planes_ gestionados, se necesitará el _kubeconfig_ del proveedor). Para asegurar este paso, se buscará el siguiente texto en la salida del comando: "Moving Cluster API objects Clusters=1".
+
[source,bash]
----
[bastion]$ sudo clusterctl --kubeconfig $KUBECONFIG move -n cluster-<cluster_name> --to-kubeconfig /root/.kube/config
----

. Accede al _cluster_ local y elimina el _cluster_ _worker_.
+
[source,bash]
----
[bastion]$ sudo docker exec -ti <cluster_name>-control-plane bash
root@<cluster_name>-control-plane:/# kubectl -n cluster-<cluster_name> delete cl --all
cluster.cluster.x-k8s.io "<cluster_name>" deleted
root@<cluster_name>-control-plane:/#
----

. Finalmente, elimina el _cluster_ local.
+
[source,bash]
----
[bastion]$ sudo ./bin/cloud-provisioner delete cluster --name <cluster_name>
----

== Instalación _offline_

Para saber cómo llevar a cabo una instalación en la que las imágenes de los _workloads_ del _cluster_ provengan de repositorios accesibles desde entornos sin acceso a internet, consulta el xref:operations-manual:offline-installation.adoc[Manual de instalación _offline_].

== Gestión de credenciales

Para administrar las credenciales configuradas en el _cluster_, consulta la xref:operations-manual:credentials.adoc[documentación de gestión de credenciales].
