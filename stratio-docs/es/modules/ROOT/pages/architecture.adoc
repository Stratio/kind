= Arquitectura

Arquitectura de referencia:

image::eks-reference-architecture.png[]

== Fases de aprovisionamiento

image::arq-intro.png[]

== Introducción

_Stratio Cloud Provisioner_ es la fase inicial para la creación de un _cluster_ de _Stratio KEOS_ en un proveedor _cloud_. Esta fase comprende el aprovisionamiento de la infraestructura (máquinas virtuales, redes privadas, balanceadores de carga, etc.), la creación de un _cluster_ de Kubernetes, su _networking_ y almacenamiento, todo ello en el proveedor seleccionado.

Durante esta fase, se genera un recurso de Kubernetes denominado _KeosCluster_, el cual define las características del _cluster_ y actúa como punto único de gestión de su ciclo de vida. A partir del descriptor de _cluster_ proporcionado, también se generan dos archivos: un descriptor (`keos.yaml`) y otro cifrado con las credenciales (`secrets.yml`) los cuales serán utilizados en la siguiente fase de instalación de _Stratio KEOS_.

Una vez completada la instalación, todas las operaciones de mantenimiento y gestión de la infraestructura _cloud_ (día 2) deberán realizarse exclusivamente mediante la edición del recurso _KeosCluster_, garantizando así una administración controlada por el _Stratio Cluster Operator_.

== Recurso _KeosCluster_

Tras desplegar el _chart_ de Helm del _Stratio Cluster Operator_ y con base en el descriptor del _cluster_, se crea automáticamente un recurso _KeosCluster_. Este recurso centraliza la creación y administración del _cluster_ y los objetos específicos del proveedor _cloud_.

=== Control de concurrencia y prevención de errores

Para evitar conflictos y mitigar errores humanos, el _Stratio Cluster Operator_ deniega nuevas solicitudes si ya existe una operación en curso sobre el objeto _KeosCluster_. Esta situación se refleja mediante su subrecurso `status`, que informa del tipo de operación activa.

== Recurso _ClusterConfig_

Junto con el objeto _KeosCluster_, se puede declarar un recurso adicional llamado _ClusterConfig_, el cual permite definir configuraciones específicas para el _cluster_.

La definición de este recurso debe incluirse en el mismo fichero descriptor que el _KeosCluster_. Si no se declara explícitamente durante la instalación, se generará automáticamente con valores por defecto.

TIP: Para más detalles, consulta la sección de xref:operations-manual:api-reference.adoc[referencia API].

=== Selección del _Cluster Operator_

Por defecto, se instalará la última versión del _chart_ disponible en el repositorio de Helm especificado en el objeto _KeosCluster_. Este comportamiento puede sobrescribirse indicando manualmente la versión deseada en la configuración del _ClusterConfig_.

TIP: Para más detalles, consulta la sección de xref:operations-manual:api-reference.adoc[referencia API].

==== Criterios de selección de versión

La elección automática de la versión se basa en los siguientes criterios:

- Prioridad de versionado: _release_, _prerelease_, _milestone_, _snapshot_, _pull request_.
- Orden alfanumérico dentro de versiones de igual precedencia (excepto _prerelease_).

Por ejemplo, se seleccionará `0.2.1` antes que `0.2.0`. En el caso de versiones _prerelease_ sin un orden alfanumérico claro, se seleccionará la última versión publicada en el repositorio de Helm, debido al esquema de nomenclatura utilizado en Stratio.

NOTE: Si se utiliza la selección por defecto, no es necesario especificar la versión manualmente. Esta se reflejará automáticamente en el objeto creado en el _cluster_.

== Objetos del proveedor del _cloud_

En un *despliegue por defecto* se crean los siguientes objetos en cada proveedor _cloud_ (los objetos generados dependerán de lo especificado en el descriptor del _cluster_):

=== EKS

[cols="2,1", options="header"]
|===
| Recurso
| Cantidad

| Cluster de Elastic Kubernetes Service (EKS)
| 1

| Add-ons gestionados por EKS (EBS CSI Driver, CoreDNS, kube-proxy, VPC CNI)
| 1 conjunto

| Add-on de logging (CloudWatch)
| 1 (si se especifica)

| Proveedor OIDC
| 1

| VPC*
| 1

| Subnets públicas (una por zona de disponibilidad)
| 3

| Subnets privadas (una por zona de disponibilidad)
| 3

| Internet Gateway*
| 1

| NAT Gateways (uno por subnet pública)
| 3

| Tablas de rutas con salida a Internet Gateway
| 3

| Tablas de rutas con salida a NAT Gateway
| 3

| Security Groups (control-plane y nodos _Worker_)
| 2

| IAM Role `<cluster-name>-iam-service-role` para el _control-plane_
| 1

| Asociar IAM Policy `AmazonEKSClusterPolicy` al rol `<cluster-name>-iam-service-role`
| 1

| IAM Role para _nodos del cluster_ `nodes.cluster-api-provider-aws.sigs.k8s.io`
| 1

| IAM Policy para nodos (`nodes.cluster-api-provider-aws.sigs.k8s.io`)
| 1

| Máquinas virtuales para nodos _Worker_
| Según descriptor y autoescalado

| Volúmenes EBS persistentes
| 1 por volumen solicitado

| Balanceador de carga tipo _Network_
| 1

| Listeners (uno por puerto de cada _Service_)
| Variable
|===

[IMPORTANT]
.Dependencias según permisos del cliente
====
Dependiendo de los permisos disponibles en la cuenta del cliente, será necesario crear previamente los siguientes roles y políticas como prerrequisitos. 

En caso contrario, estos se crearán automáticamente durante el proceso:

* Rol: `nodes.cluster-api-provider-aws.sigs.k8s.io`
* Política: `nodes.cluster-api-provider-aws.sigs.k8s.io` y asociarla al rol `nodes.cluster-api-provider-aws.sigs.k8s.io`
* Rol: `<cluster-name>-iam-service-role (controlplane)`
* Política: `AmazonEKSClusterPolicy` y asociarla al rol `<cluster-name>-iam-service-role`
====

=== GKE (_cluster_ privado)

[cols="2,1", options="header"]
|===
| Recurso
| Cantidad

| Cluster de Google Kubernetes Engine (GKE) configurado con VPC-nativa
| 1

| VPC
| 1

| Subred por región
| 1

| Bloque CIDR principal para subred (nodos)
| 1

| Bloque CIDR secundario para subred (pods y servicios)
| 1 por tipo

| Ruta de peering (VPC Network Peering)
| 1

| Rutas para bloques CIDR secundarios (pods y servicios)
| 2

| Red de VPC peering
| 1

| Reglas de firewall de VPC  
(gke-<nombre-cluster>-<id>-[master, vms, exkubelet, inkubelet, all])
| 5

| Máquinas virtuales para nodos _Worker_
| Según descriptor y autoescalado

| Volúmenes persistentes
| 1 por nodo
|===

=== Azure no gestionado

[cols="2,1", options="header"]
|===
| Recurso
| Cantidad

| Resource Group
| 1

| Red virtual (Virtual Network)
| 1

| Route table para nodos _Worker_
| 1

| NAT Gateway para nodos _Worker_
| 1

| Direcciones IP públicas (API Server y NAT Gateway)
| 2

| Grupos de seguridad de red (NSG) para _control-plane_ y _workers_
| 2

| Balanceador de carga público para API Server
| 1

| Máquinas virtuales para _control-plane_
| 1–3 (según descriptor)

| Disco de bloque por máquina virtual de _control-plane_
| 1 por VM

| Interfaz de red por máquina virtual de _control-plane_
| 1 por VM

| Máquinas virtuales para nodos _Worker_
| Según descriptor y autoescalado

| Disco de bloque por máquina virtual de _Worker_
| 1 por VM

| Interfaz de red por máquina virtual de _Worker_
| 1 por VM

| Balanceador de carga para exposición de _Services_ tipo LoadBalancer
| 1

| Dirección IP pública por _Service_ expuesto
| 1 por Service

| Configuración de IP frontal (_Frontend IP config_) por _Service_
| 1 por Service

| _Health probe_ por _Service_
| 1 por Service

| Regla de balanceador de carga por _Service_
| 1 por Service

| Disco de bloque para volúmenes persistentes
| 1 por volumen solicitado
|===

== _Networking_

Arquitectura de referencia:

image::eks-reference-architecture.png[]

La capa interna de _networking_ del _cluster_ está basada en Calico, con las siguientes integraciones por _proveedor_:

[.center,cols="1,1,1,1,1,1",center]
|===
^|Proveedor ^|Política ^|IPAM ^|CNI ^|Superposición ^|Enrutamiento

^|*EKS*
^|Calico
^|AWS
^|AWS
^|No
^|VPC-native

^|*GKE*
^|Calico
^|Calico
^|Calico
^|No
^|VPC-nativa

^|*Azure*
^|Calico
^|Calico
^|Calico
^|VxLAN
^|Calico
|===

=== Infraestructura propia

Si bien una de las ventajas de la creación de recursos automática en el aprovisionamiento es el gran dinamismo que otorga, por motivos de seguridad y cumplimiento de normativas, muchas veces es necesario crear ciertos recursos previamente al despliegue de _Stratio KEOS_ en el proveedor de _Cloud_.

En este sentido, _Stratio Cloud Provisioner_ permite utilizar tanto un VPC como _subnets_ previamente creadas empleando el parámetro _networks_ en el descriptor del _cluster_, como se detalla en la xref:operations-manual:installation.adoc[guía de instalación].

Ejemplo para EKS:

[source,bash]
----
spec:
  networks:
    vpc_id: vpc-02698..
    subnets:
      - subnet_id: subnet-0416d..
      - subnet_id: subnet-0b2f8..
      - subnet_id: subnet-0df75..
----

=== Red de _pods_

En la mayoría de _proveedores_ se permite indicar un CIDR específico para _pods_, con ciertas particularidades descritas a continuación.

NOTE: El CIDR para _pods_ no deberá superponerse con la red de los nodos o cualquier otra red destino a la que éstos deban acceder.

==== EKS

En este caso, y dado que se utiliza el AWS VPC CNI como IPAM, se permitirá sólo uno de los dos rangos soportados por EKS: 100.64.0.0/16 o 198.19.0.0/16 (siempre teniendo en cuenta las restricciones de la https://docs.aws.amazon.com/vpc/latest/userguide/vpc-cidr-blocks.html#add-cidr-block-restrictions[documentación oficial]), que se añadirán al VPC como _secondary CIDR_.

NOTE: Si no se indica infraestructura _custom_, se deberá utilizar el CIDR 100.64.0.0/16.

[source,bash]
----
spec:
  networks:
	  pods_cidr: 100.64.0.0/16
----

En este caso, se crearán 3 _subnets_ (1 por zona) con una máscara de 18 bits (/18) del rango indicado de las cuales se obtendrán las IP para los _pods_:

[.center,cols="1,2",width=40%, options="header"]
|===
^|**Zona**
^|**CIDR**

^|zone-a
^|100.64.0.0/18

^|zone-b
^|100.64.64.0/18

^|zone-c
^|100.64.128.0/18
|===

NOTE: El CIDR secundario asignado al VPC para los _pods_ debe indicarse en el parámetro `spec.networks.pods_cidr` obligatoriamente.

En caso de utilizar infraestructura personalizada, se deberán indicar las 3 _subnets_ (una por zona) para los _pods_ conjuntamente con las de los nodos en el descriptor del _cluster_:

[source,bash]
----
spec:
  networks:
      vpc_id: vpc-0264503b4f41ff69f # example-custom-vpc
      pods_subnets:
          - subnet_id: subnet-0f6aa193eaa31015e # example-custom-sn-pods-zone-a
          - subnet_id: subnet-0ad0a80d1cec762d7 # example-custom-sn-pods-zone-b
          - subnet_id: subnet-0921f337cb6a6128d # example-custom-sn-pods-zone-c
      subnets:
          - subnet_id: subnet-0416da6767f910929 # example-custom-sn-priv-zone-a
          - subnet_id: subnet-0b2f81b89da1dfdfd # example-custom-sn-priv-zone-b
          - subnet_id: subnet-0df75719efe5f6615 # example-custom-sn-priv-zone-c
----

El CIDR de cada subnet (obtenido del CIDR secundario del VPC), deberá ser el mismo que el descrito más arriba (con máscara de 18 bits), y las 3 _subnets_ para _pods_ deberán tener el siguiente tag: _sigs.k8s.io/cluster-api-provider-aws/association=secondary_.

==== Azure no gestionado

En este proveedor/_flavour_ se utiliza Calico como IPAM del CNI, esto permite poder especificar un CIDR arbitrario para los _pods_:

[source,bash]
----
spec:
  networks:
	  pods_cidr: 100.64.0.0/10
----

NOTE: Nuestra recomendación es usar bloques 100.64.0.0/10, 172.16.0.0/12, o 192.168.0.0/16 para pods_cidr si no se solapan con la VNet del _cluster_.
El rango 100.64.0.0/10 es altamente recomendado para evitar conflictos con RFC1918.

==== GKE

La red de _pods_ en GKE se configura automáticamente con el CIDR secundario para _pods_ y servicios, obtenido de la configuración de la red VPC al desplegar el _cluster_.

Para especificar una red de _pods_ diferente, podremos hacerlo de 2 formas excluyentes entre sí:

* Pre-creando los rangos CIDR en la subnet de la red VPC y especificando el CIDR en el descriptor del _cluster_.

[source,bash]
----
spec:
  control_plane:
          managed: true
          gcp:
              ip_allocation_policy:
                  cluster_secondary_range_name: "gkepods-europe-west4"
                  services_secondary_range_name: "gkeservices-europe-west4"
----

* Indicando el CIDR en el descriptor del _cluster_ y dejando que GKE lo cree automáticamente.

[source,bash]
----
spec:
  control_plane:
        managed: true
        gcp:
            ip_allocation_policy:
                cluster_ipv4_cidr_block: 10.180.0.0/14
                services_ipv4_cidr_block: 10.8.32.0/20
----

NOTE: La elección de un bloque CIDR para _pods_ y servicios en GKE es opcional, ya que si no se especifica, GKE asignará automáticamente un bloque CIDR a la red de _pods_ y otro a la de servicios.

NOTE: El rango de _pods_ no debe solaparse con el bloque CIDR de la red VPC o cualquier otra red a la que los nodos deban acceder.

== Seguridad

=== Autenticación

Actualmente, para la comunicación con los proveedores _cloud_, los _controllers_ almacenan en el _cluster_ las credenciales de la identidad utilizada en la instalación.

==== EKS

Para este proveedor, las credenciales se almacenan en un _Secret_ dentro del _Namespace_ del _controller_ (capa-system/capi-system/kube-system) utilizando el formato estándar de configuración de credenciales de AWS (`~/.aws/credentials`), que sigue la especificación de perfiles de AWS CLI.

A continuación se muestra una tabla con los controladores utilizados y la ubicación de sus credenciales:

[cols="3,2,1,1,1,1", options="header"]
|===
| Controlador
| ServiceAccount
| Nombre del Secret
| Cifrado
| Tipo de autenticación
| Flujo de autenticación

| `capa-controller-manager`
| `capa-controller-manager`
| `capa-manager-bootstrap-credentials`
| Sí (base64)
| Credenciales de AWS
| OAuth2 Client Credentials

| `capa-controller-manager`
| `capa-controller-manager`
| `capa-webhook-service-cert`
| Sí (base64)
| kubernetes.io/tls
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-bootstrap-controller-manager`
| `capi-kubeadm-bootstrap-manager`
| `capi-kubeadm-bootstrap-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-control-plane-controller-manager`
| `capi-kubeadm-control-plane-manager`
| `capi-kubeadm-control-plane-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-controller-manager`
| `capi-manager`
| `capi-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `keoscluster-settings`
| Sí (base64)
| ClientID + Secret (Azure)
| OAuth2 Client Credentials

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `webhook-server-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (ValidatingAdmissionWebhook)

|===

Para ver el contenido de las credenciales, se puede utilizar el siguiente comando a modo de ejemplo:

[source,bash]
----
k -n capa-system get secret capa-manager-bootstrap-credentials -o json | jq -r '.data.credentials' | base64 -d

[default]
aws_access_key_id = XXXXXXXXXXXXXXXXXXXXXXX
aws_secret_access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
region = eu-west-1
----

==== GKE

Los _controllers_ de GKE almacenan las credenciales en un _Secret_ dentro del _Namespace_ del _controller_ (capg-system/capi-system/kube-system) utilizando el formato estándar de configuración de credenciales de GCP (`~/.gcloud/config`), que sigue la especificación de perfiles de GCP.

A continuación se muestra una tabla con los controladores utilizados y la ubicación de sus credenciales:

[cols="3,2,1,1,1,1", options="header"]
|===
| Controlador
| ServiceAccount
| Nombre del Secret
| Cifrado
| Tipo de autenticación
| Flujo de autenticación

| `capg-controller-manager`
| `capg-manager`
| `capg-manager-bootstrap-credentials`
| Sí (base64)
| Credenciales de GCP
| OAuth2 Client Credentials

| `capg-controller-manager`
| `capg-manager`
| `capg-webhook-service-cert`
| Sí (base64)
| kubernetes.io/tls
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-bootstrap-controller-manager`
| `capi-kubeadm-bootstrap-manager`
| `capi-kubeadm-bootstrap-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-control-plane-controller-manager`
| `capi-kubeadm-control-plane-manager`
| `capi-kubeadm-control-plane-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-controller-manager`
| `capi-manager`
| `capi-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `keoscluster-settings`
| Sí (base64)
| ClientID + Secret (Azure)
| OAuth2 Client Credentials

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `webhook-server-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (ValidatingAdmissionWebhook)
|===

[source,bash]
----
$ k -n capg-system get secret capg-manager-bootstrap-credentials -o json | jq -r '.data["credentials.json"]' | base64 -d | jq .
----

==== Azure

Los _controllers_ de Azure almacenan las credenciales en un _Secret_ dentro del _Namespace_ del _controller_ (capz-system/capi-system/kube-system) utilizando el formato estándar de configuración de credenciales de Azure (`~/.azure/credentials`), que sigue la especificación de perfiles de Azure.

[cols="3,2,1,1,1,1", options="header"]
|===
| Controlador
| ServiceAccount
| Nombre del Secret
| Cifrado
| Tipo de autenticación
| Flujo de autenticación

| `capz-controller-manager`
| `capz-manager`
| `cluster-identity-secret`
| Sí (base64)
| ClientID + Secret
| OAuth2 Client Credentials

| `capz-controller-manager`
| `capz-manager`
| `capz-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-bootstrap-controller-manager`
| `capi-kubeadm-bootstrap-manager`
| `capi-kubeadm-bootstrap-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-control-plane-controller-manager`
| `capi-kubeadm-control-plane-manager`
| `capi-kubeadm-control-plane-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-controller-manager`
| `capi-manager`
| `capi-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `keoscluster-settings`
| Sí (base64)
| ClientID + Secret (Azure)
| OAuth2 Client Credentials

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `webhook-server-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (ValidatingAdmissionWebhook)
|===

Para ver el contenido de las credenciales, se puede utilizar el siguiente comando a modo de ejemplo:

[source,bash]
----
k -n capz-system get secret cluster-identity-secret -o json | jq -r '.data["clientSecret"]' | base64 -d
----

NOTE: Para actualizar las credenciales del _keoscluster-controller-manager_ o de los controladores _capa_, _capg_ o _capz_, consulta la sección correspondiente en la guía de xref:operations-manual:credentials.adoc[Renovación de credenciales].

=== Acceso a IMDS

==== EKS (IMDSv2)

Dado que los _pods_ pueden impersonar al nodo donde se ejecutan simplemente interactuando con IMDS, se utiliza una política de red global (_GlobalNetworkPolicy_ de Calico) para impedir el acceso a todos los _pods_ del _cluster_ que no sean parte de _Stratio KEOS_.

A su vez, en EKS se habilita el proveedor OIDC para permitir el uso de roles de IAM para _Service Accounts_, asegurando el uso de políticas IAM con mínimos privilegios.

Para verificar la configuración de IMDSv2, se puede utilizar el siguiente comando:

[source,bash]
----
# Obtener los IDs de todas las instancias asociadas al cluster
INSTANCE_IDS=$(aws ec2 describe-instances \
  --filters "Name=tag:kubernetes.io/cluster/<cluster-name>,Values=owned" \
  --query "Reservations[*].Instances[*].InstanceId" \
  --output text)

# Verificar la configuración de IMDSv2 para cada instancia
for ID in $INSTANCE_IDS; do
  echo "Verificando instancia $ID:"
  aws ec2 describe-instances \
    --instance-ids "$ID" \
    --query "Reservations[*].Instances[*].MetadataOptions" \
    --output json
done
----

==== GKE (IMDSv2)

En Google Kubernetes Engine (GKE), los nodos del clúster acceden al servidor de metadatos (`IMDS`) a través de la IP reservada `169.254.169.254`. Este servidor permite obtener credenciales mediante Application Default Credentials (ADC), que los _pods_ también pueden utilizar si no se restringe explícitamente.

Dado que los _pods_ pueden impersonar al nodo donde se ejecutan accediendo directamente a `IMDS`, se utiliza una política de red global (`GlobalNetworkPolicy` de Calico) para impedir el acceso a este endpoint desde cualquier _pod_ que no pertenezca a componentes autorizados, como los controladores de infraestructura de Stratio KEOS.

Esta política filtra el tráfico de salida (`egress`) hacia `169.254.169.254` mediante un selector de _labels_, limitando el acceso únicamente a los _pods_ que requieren interacción con las APIs de GCP y están adecuadamente identificados.

==== Azure no gestionado



=== Acceso al _endpoint_ del _API Server_

==== EKS

Durante la creación del _cluster_ de EKS, se crea un _endpoint_ para el _API Server_ que se utilizará para el acceso al _cluster_ desde el instalador y operaciones del ciclo de vida.

Este _endpoint_ se publica a internet y de forma privada, y su acceso se restringe con una combinación de reglas del _Identity and Access Management_ (IAM) de Amazon y el _Role Based Access Control_ (RBAC) nativo de Kubernetes.

Para comprobar la creación y el tipo de acceso del _endpoint_, se pueden utilizar los siguientes comandos:

[source,bash]
----
# Comprobar la creaación del _endpoint_:
aws eks describe-cluster --name <cluster_name> --query "cluster.endpoint" --output text | cat 
https://XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.gr7.eu-west-1.eks.amazonaws.com
# Comprobar el tipo de acceso:
aws eks describe-cluster --name eks-cl02 --query "cluster.resourcesVpcConfig" --output json | cat

    "subnetIds": [
        "subnet-0cd582b2fc8f4667f",
        "subnet-036599062ce4633b4",
        "subnet-0ed8d484e85078953",
        "subnet-0e33205cc1afeb1ae",
        "subnet-01299725d68bc6a10",
        "subnet-0764ad7f79ecee088"
    ],
    "securityGroupIds": [
        "sg-XXXXXXXXXXXXXXXXX"
    ],
    "clusterSecurityGroupId": "sg-XXXXXXXXXXXXXXXXX",
    "vpcId": "vpc-XXXXXXXXXXXXXXXXX",
    "endpointPublicAccess": true,   # Acceso público habilitado
    "endpointPrivateAccess": true,  # Acceso privado habilitado
    "publicAccessCidrs": [
        "0.0.0.0/0"
    ]
}

----

==== GKE

En este caso, el _API Server_ se expone únicamente de forma privada, por lo que solo se puede acceder desde la IP asignada al _endpoint_ privado del _cluster_. Esta IP pertenece al rango especificado en el descriptor del _cluster_.

Para comprobar la creación y el tipo de acceso del _endpoint_, se pueden utilizar los siguientes comandos:
[source,bash]
----
# Comprobar la creación del _endpoint_:
gcloud container clusters describe <cluster_name> --region <region> --format="get(privateClusterConfig.privateEndpoint)"
172.16.16.2
# Comprobar el tipo de acceso:
gcloud container clusters describe <cluster_name> --region <region> --format="get(privateClusterConfig.enablePrivateEndpoint)"
True
----

==== Azure no gestionado

Para la exposición del _API Server_, se crea un balanceador de carga con nombre `<cluster_id>-public-lb` y puerto 6443 accesible por red pública (la IP pública asignada es la misma que resuelve la URL del _Kubeconfig_) y un _Backend pool_ con los nodos del _control-plane_.

El _health check_ del servicio se hace por TCP, pero se recomienda cambiar a HTTPS con la ruta `/healthz`.

Para validar la exposición del API Server en Azure, se pueden utilizar los siguientes comandos:
[source,bash]
----
# Comprobar la creación del _endpoint_:
az network lb list -g <resource_group> --query "[].{Name:name, PublicIP:frontendIpConfigurations[].publicIpAddress.id}" -o table
Name
----------------
azure-public-lb
# Comprobar la ip de exposición:
az network public-ip list -g <resource_group> \
  --query "[?ipConfiguration.id && contains(ipConfiguration.id, '<load_balancer_name>')].{Name:name, IP:ipAddress}" \
  -o table
Name                  IP
--------------------  -------------
pip-azure-apiserver  132.164.7.182
----

== Almacenamiento

=== Nodos (_control-plane_ y _workers_)

A nivel de almacenamiento, se monta un único disco _root_ del que se puede definir su tipo, tamaño y encriptación (se podrá especificar una clave de encriptación previamente creada).

*Ejemplo:*

[source,bash]
----
type: gp3
size: 384Gi
encrypted: true
encryption_key: <key_name>
----

Estos discos se crean en la provisión inicial de los nodos, por lo que estos datos se pasan como parámetros del descriptor.

=== _StorageClass_

Durante el aprovisionamiento se disponibiliza una _StorageClass_ (por defecto) con nombre "keos" para disco de bloques. Esta cuenta con los parámetros `reclaimPolicy: Delete` y `volumeBindingMode: WaitForFirstConsumer`, esto es, que el disco se creará en el momento en que un _pod_ consuma el _PersistentVolumeClaim_ correspondiente y se eliminará al borrar el _PersistentVolume_.

NOTE: Ten en cuenta que los _PersistentVolumes_ creados a partir de esta _StorageClass_ tendrán afinidad con la zona donde se han consumido.

Desde el descriptor del _cluster_ se permite indicar la clave de encriptación, la clase de discos o bien parámetros libres.

*Ejemplo con opciones básicas:*

[source,bash]
----
spec:
  infra_provider: aws
  storageclass:
    encryption_key: <my_simm_key>
    class: premium
----

El parámetro `class` puede ser _premium_ o _standard_, esto dependerá del proveedor _cloud_:

[.center,cols="1,2,2",width=70%,center]
|===
^|Proveedor ^|Standard class ^|Premium class

^|AWS
^|gp3
^|io2 (64k IOPS)

^|GKE
^|pd-standard
^|pd-ssd

^|Azure
^|StandardSSD_LRS
^|Premium_LRS
|===

*Ejemplo con parámetros libres:*

[source,bash]
----
spec:
  infra_provider: gcp
  storageclass:
    parameters:
      type: pd-extreme
      provisioned-iops-on-create: 5000
      disk-encryption-kms-key: <key_name>
      labels: "key1=value1,key2=value2"
----

Estos últimos también dependen del proveedor _cloud_:

[.center,cols="1,2",width=80%]
|===
^|Proveedor ^|Parámetro

^|All
a|

----
     fsType
----

^|AWS, GKE
a|

----
     type
     labels
----

^|AWS
a|

----
     iopsPerGB
     kmsKeyId
     allowAutoIOPSPerGBIncrease
     iops
     throughput
     encrypted
     blockExpress
     blockSize
----

^|GKE
a|

----
     provisioned-iops-on-create
     replication-type
     disk-encryption-kms-key
----

^|Azure
a|

----
     provisioner
     skuName
     kind
     cachingMode
     diskEncryptionType
     diskEncryptionSetID
     resourceGroup
     tags
     networkAccessPolicy
     publicNetworkAccess
     diskAccessID
     enableBursting
     enablePerformancePlus
     subscriptionID
----

|===

En el aprovisionamiento se crean otras _StorageClasses_ (no default) según el proveedor, pero para utilizarlas, las cargas de trabajo deberán especificarlas en su despliegue.

== Atributos en EKS

Todos los objetos que se crean en EKS contienen por defecto el atributo con clave _keos.stratio.com/owner_ y como valor el nombre del _cluster_. También se permite añadir atributos personalizados a todos los objetos creados en el proveedor _cloud_ de la siguiente forma:

[source,bash]
----
spec:
  control_plane:
    tags:
      - tier: production
      - billing-area: data
----

Para añadir atributos a los volúmenes creados por la _StorageClass_, se deberá utilizar el parámetro `labels` en la sección correspondiente:

[source,bash]
----
spec:
  storageclass:
    parameters:
      labels: "tier=production,billing-area=data"
      ..
----

== Docker registries

Como prerrequisito a la instalación de _Stratio KEOS_, las imágenes Docker de todos sus componentes deberán residir en un Docker registry que se indicará en el descriptor del _cluster_ (`keos_registry: true`). Deberá haber un (y sólo uno) Docker registry para _Stratio KEOS_, el resto se configurarán en los nodos para poder utilizar sus imágenes en cualquier despliegue.

Actualmente, se soportan 3 tipos de Docker registries: _acr_, _ecr_, _gar_ y _generic_. Para el tipo _generic_, se deberá indicar si el _registry_ es autenticado o no (los tipos _ecr_, _acr_ y _gar_ no pueden tener autenticación), y en caso de serlo, es obligatorio indicar usuario y contraseña en la sección 'spec.credentials'.

La siguiente tabla muestra los _registries_ soportados según _proveedor_:

[.center,cols="2,1",width=40%]
|===
^|EKS
^|ecr, generic

^|Azure
^|acr, generic

^|GKE
^|gar
|===

== Repositorio de Helm

Como prerrequisito de la instalación, se debe indicar un repositorio de Helm del que se pueda extraer el _chart_ del _Cluster Operator_. Este repositorio puede utilizar protocolos HTTPS u OCI (utilizados para repositorios de proveedores _cloud_ como ECR, GAR o ACR).

[.center,cols="2,1",width=40%]
|===
^|EKS
^|ecr, generic

^|Azure
^|acr, generic

^|GKE
^|gar
|===

NOTE: Las URL de los repositorios de tipo OCI llevan el prefijo *oci://*. Por ejemplo: oci://stratioregistry.azurecr.io/helm-repository-example.

NOTE: Recuerda verificar en la documentación de _keos-installer_ los repositorios que se soporten en la versión a utilizar.
