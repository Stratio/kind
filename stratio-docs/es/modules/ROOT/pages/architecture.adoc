= Arquitectura

Arquitectura de referencia:

image::eks-reference-architecture.png[]

== Fases de aprovisionamiento

image::arq-intro.png[]

== Introducción

_Stratio Cloud Provisioner_ es la primera fase para crear un _cluster_ de _Stratio KEOS_ en un proveedor _cloud_. Esta fase incluye el aprovisionamiento de la infraestructura (máquinas virtuales, redes, balanceadores de carga, etc.), la creación del _cluster_ de Kubernetes, su red y almacenamiento, todo ello en el proveedor _cloud_ que se haya elegido.

Durante esta fase se genera un recurso de Kubernetes llamado _KeosCluster_, que define las características del _cluster_ y actúa como único punto de gestión de su ciclo de vida. A partir del descriptor del _cluster_ proporcionado, también se crean dos archivos: un descriptor (_keos.yaml_) y otro con las credenciales cifradas (_secrets.yml_). Ambos se usarán en la siguiente fase de instalación de _Stratio KEOS_.

Una vez instalado el _cluster_, todas las tareas de mantenimiento y gestión de la infraestructura _cloud_ (día 2) deben hacerse exclusivamente editando el recurso _KeosCluster_. Esto garantiza una administración controlada por el _Stratio Cluster Operator_.

== Recurso _KeosCluster_

Cuando se despliega el _chart_ de Helm del _Stratio Cluster Operator_ y se proporciona un descriptor del _cluster_, se crea automáticamente un recurso _KeosCluster_. Este recurso centraliza tanto la creación como la gestión del _cluster_ y de los recursos específicos del proveedor _cloud_.

=== Control de concurrencia y prevención de errores

Para evitar conflictos y mitigar errores humanos, el _Stratio Cluster Operator_ bloquea nuevas operaciones si ya hay una en curso sobre el objeto _KeosCluster_. Esta situación se refleja en el subrecurso `status`, que indica el tipo de operación activa.

== Recurso _ClusterConfig_

Junto con el recurso _KeosCluster_, puede definirse otro llamado _ClusterConfig_, que permite establecer configuraciones adicionales para el _cluster_.

Este recurso debe declararse en el mismo archivo que el _KeosCluster_. Si no se especifica durante la instalación, se generará automáticamente con valores por defecto.

TIP: Para más información, consulta la sección xref:operations-manual:api-reference.adoc[referencia API].

=== Selección del _Cluster Operator_

Por defecto, se instalará la última versión del _chart_ disponible en el repositorio de Helm indicado en el objeto _KeosCluster_. Es posible modificar este comportamiento especificando manualmente la versión deseada en la configuración del _ClusterConfig_.

TIP: Para más información, consulta la sección xref:operations-manual:api-reference.adoc[referencia API].

==== Criterios de selección de versión

La selección automática de la versión se basa en los siguientes criterios:

- Prioridad del tipo de versión: _release_, _prerelease_, _milestone_, _snapshot_, _pull request_.
- Orden alfanumérico dentro de versiones del mismo tipo (excepto _prerelease_).

Por ejemplo, se prioriza la versión `0.2.1` frente a `0.2.0`. En el caso de versiones _prerelease_ sin un orden claro, se elige la última publicada en el repositorio de Helm, según el esquema de nombres usado en Stratio.

NOTE: Si se usa la selección por defecto, no es necesario indicar manualmente la versión. Esta quedará reflejada en el objeto creado dentro del _cluster_.

== Recursos generados por proveedor _cloud_

En un *despliegue por defecto*, se crean los siguientes objetos en cada proveedor _cloud_. Los objetos concretos dependen del contenido del descriptor del _cluster_).

=== EKS

[cols="2,1", options="header"]
|===
| Recurso
| Cantidad

| _Cluster_ de Amazon EKS
| 1

| _Add-ons_ gestionados por EKS (EBS CSI Driver, CoreDNS, kube-proxy, VPC CNI)
| 1 conjunto

| _Add-ons_ de registro de _logs_ (CloudWatch)
| 1 (si se especifica)

| Proveedor OIDC
| 1

| VPC
| 1

| Subredes públicas (una por zona de disponibilidad)
| 3

| Subredes privadas (una por zona de disponibilidad)
| 3

| Internet Gateway
| 1

| NAT Gateways (uno por subred pública)
| 3

| Tablas de rutas con salida a Internet Gateway
| 3

| Tablas de rutas con salida a NAT Gateway
| 3

| Grupos de seguridad (para _control-plane_ y nodos _Worker_)
| 2

| Rol IAM `<cluster-name>-iam-service-role` para el _control-plane_
| 1

| Política IAM  `AmazonEKSClusterPolicy` asociada al rol anterior
| 1

| Rol IAM para nodos del _cluster_ `nodes.cluster-api-provider-aws.sigs.k8s.io`
| 1

| Política IAM para los nodos `nodes.cluster-api-provider-aws.sigs.k8s.io`
| 1

| Máquinas virtuales para nodos _Worker_
| Según descriptor y autoescalado

| Volúmenes EBS persistentes
| 1 por volumen solicitado

| Balanceador de carga tipo _Network_
| 1

| Listeners (uno por puerto de cada _Service_)
| Variable
|===

[IMPORTANT]
.Requisitos según permisos del cliente
====
Según los permisos de la cuenta _cloud_, puede ser necesario crear previamente los siguientes roles y políticas. En caso contrario, se crearán automáticamente durante el proceso:

* Rol: `nodes.cluster-api-provider-aws.sigs.k8s.io`
* Política: `nodes.cluster-api-provider-aws.sigs.k8s.io`, asociada al rol anterior.
* Rol: `<cluster-name>-iam-service-role (controlplane)`
* Política: `AmazonEKSClusterPolicy` (ya existente en Amazon), asociada al rol anterior.
** xref:attachment$nodes-cluster-api-provider-aws-sigs-k8s-io.json[Descargar política de nodos EKS]
** xref:attachment$nodes-trust-relationship.json[Descargar política de relación de confianza de nodos EKS]
====

=== GKE (_cluster_ privado)

[cols="2,1", options="header"]
|===
| Recurso
| Cantidad

| _Cluster_ de Google Kubernetes Engine (GKE) con red VPC nativa
| 1

| VPC
| 1

| Subred por región
| 1

| Bloque CIDR principal (para nodos)
| 1

| Bloque CIDR secundario (para _pods_ y servicios)
| 1 por tipo

| Ruta de _peering_ (VPC Network Peering)
| 1

| Rutas para bloques CIDR secundarios (pods y servicios)
| 2

| Red de VPC _peering_
| 1

| Reglas de _firewall_ en la VPC
(gke-<nombre-cluster>-<id>-[master, vms, exkubelet, inkubelet, all])
| 5

| Máquinas virtuales para nodos _Worker_
| Según descriptor y autoescalado

| Volúmenes persistentes
| 1 por nodo
|===

=== Azure no gestionado

[cols="2,1", options="header"]
|===
| Recurso
| Cantidad

| _Resource Group_
| 1

| Red virtual (_Virtual Network_)
| 1

| Tabla de rutas para nodos _Worker_
| 1

| NAT Gateway para nodos _Worker_
| 1

| Direcciones IP públicas (API Server y NAT Gateway)
| 2

| Grupos de seguridad de red (NSG) para _control-plane_ y _workers_
| 2

| Balanceador de carga público para el API Server
| 1

| Máquinas virtuales para el _control-plane_
| 1–3 (según descriptor)

| Disco de bloque por máquina del _control-plane_
| 1 por VM

| Interfaz de red por máquina del _control-plane_
| 1 por VM

| Máquinas virtuales para nodos _Worker_
| Según descriptor y autoescalado

| Disco de bloque por máquina del _Worker_
| 1 por VM

| Interfaz de red por máquina del _Worker_
| 1 por VM

| Balanceador de carga para exponer _Services_ tipo _LoadBalancer_
| 1

| Dirección IP pública por _Service_ expuesto
| 1 por _Service_

| Configuración de IP frontal (_Frontend IP config_) por _Service_
| 1 por _Service_

| _Health probe_ por _Service_
| 1 por _Service_

| Regla de balanceador de carga por _Service_
| 1 por _Service_

| Disco de bloque para volúmenes persistentes
| 1 por volumen solicitado
|===

== _Networking_

Arquitectura de referencia:

image::eks-reference-architecture.png[]

La capa interna de _networking_ del _cluster_ está basada en Calico, con las siguientes integraciones por proveedor:

[.center,cols="1,1,1,1,1,1",center]
|===
^|Proveedor ^|Política ^|IPAM ^|CNI ^|Superposición ^|Enrutamiento

^|*EKS*
^|Calico
^|AWS
^|AWS
^|No
^|VPC-native

^|*GKE*
^|Calico
^|Calico
^|Calico
^|No
^|VPC-nativa

^|*Azure*
^|Calico
^|Calico
^|Calico
^|VxLAN
^|Calico
|===

=== Infraestructura propia

Si bien una de las ventajas de la creación de recursos automática en el aprovisionamiento es el gran dinamismo que otorga, por motivos de seguridad y cumplimiento de normativas, muchas veces es necesario crear ciertos recursos previamente al despliegue de _Stratio KEOS_ en el proveedor de _Cloud_.

En este sentido, _Stratio Cloud Provisioner_ permite utilizar tanto un VPC como subredes (_subnets_) previamente creadas empleando el parámetro `networks` en el descriptor del _cluster_, como se detalla en la xref:operations-manual:installation.adoc[guía de instalación].

Ejemplo para EKS:

[source,bash]
----
spec:
  networks:
    vpc_id: vpc-02698..
    subnets:
      - subnet_id: subnet-0416d..
      - subnet_id: subnet-0b2f8..
      - subnet_id: subnet-0df75..
----

=== Red de _pods_

En la mayoría de proveedores se permite indicar un CIDR específico para _pods_, con ciertas particularidades descritas a continuación.

NOTE: El CIDR para _pods_ no deberá superponerse con la red de los nodos o cualquier otra red destino a la que éstos deban acceder.

==== EKS

En este caso, y dado que se utiliza el AWS VPC CNI como IPAM, se permitirá sólo uno de los dos rangos soportados por EKS: 100.64.0.0/16 o 198.19.0.0/16 (siempre teniendo en cuenta las restricciones de la https://docs.aws.amazon.com/vpc/latest/userguide/vpc-cidr-blocks.html#add-cidr-block-restrictions[documentación oficial]), que se añadirán al VPC como _secondary CIDR_.

NOTE: Si no se indica infraestructura _custom_, se deberá utilizar el CIDR 100.64.0.0/16.

[source,bash]
----
spec:
  networks:
	  pods_cidr: 100.64.0.0/16
----

En este caso, se crearán 3 _subnets_ (1 por zona) con una máscara de 18 bits (/18) del rango indicado de las cuales se obtendrán las IP para los _pods_:

[.center,cols="1,2",width=40%, options="header"]
|===
^|*Zona*
^|*CIDR*

^|zone-a
^|100.64.0.0/18

^|zone-b
^|100.64.64.0/18

^|zone-c
^|100.64.128.0/18
|===

NOTE: El CIDR secundario asignado al VPC para los _pods_ debe indicarse en el parámetro `spec.networks.pods_cidr` obligatoriamente.

En caso de utilizar infraestructura personalizada, se deberán indicar las 3 _subnets_ (una por zona) para los _pods_ conjuntamente con las de los nodos en el descriptor del _cluster_:

[source,bash]
----
spec:
  networks:
      vpc_id: vpc-0264503b4f41ff69f # example-custom-vpc
      pods_subnets:
          - subnet_id: subnet-0f6aa193eaa31015e # example-custom-sn-pods-zone-a
          - subnet_id: subnet-0ad0a80d1cec762d7 # example-custom-sn-pods-zone-b
          - subnet_id: subnet-0921f337cb6a6128d # example-custom-sn-pods-zone-c
      subnets:
          - subnet_id: subnet-0416da6767f910929 # example-custom-sn-priv-zone-a
          - subnet_id: subnet-0b2f81b89da1dfdfd # example-custom-sn-priv-zone-b
          - subnet_id: subnet-0df75719efe5f6615 # example-custom-sn-priv-zone-c
----

El CIDR de cada _subnet_ (obtenido del CIDR secundario del VPC), deberá ser el mismo que el descrito más arriba (con máscara de 18 bits), y las 3 _subnets_ para _pods_ deberán tener el siguiente tag: _sigs.k8s.io/cluster-api-provider-aws/association=secondary_.

==== Azure no gestionado

En este proveedor/_flavour_, se utiliza Calico como IPAM del CNI. Esto permite definir un bloque CIDR arbitrario para la red de _pods_, como se muestra en el siguiente ejemplo:

[source,bash]
----
spec:
  networks:
	  pods_cidr: 100.64.0.0/10
----

NOTE: Se recomienda utilizar uno de los siguientes bloques para `pods_cidr`, siempre que no se solapen con la VNet del _cluster_: `100.64.0.0/10`, `172.16.0.0/12` o `192.168.0.0/16`. El bloque `100.64.0.0/10` es especialmente recomendable, ya que no pertenece a los rangos RFC1918 y reduce el riesgo de conflictos.

==== GKE

En GKE, la red de _pods_ se configura automáticamente a partir del CIDR secundario de la red VPC definida durante la creación del _cluster_.

Si necesitas establecer manualmente una red de _pods_, puedes hacerlo de dos formas (mutuamente excluyentes):

* Precreando los rangos CIDR en la _subnet_ de la VPC y haciendo referencia a ellos en el descriptor del _cluster_.

[source,bash]
----
spec:
  control_plane:
          managed: true
          gcp:
              ip_allocation_policy:
                  cluster_secondary_range_name: "gkepods-europe-west4"
                  services_secondary_range_name: "gkeservices-europe-west4"
----

* Indicando directamente los bloques CIDR en el descriptor del _cluster_, dejando que GKE los cree automáticamente:

[source,bash]
----
spec:
  control_plane:
        managed: true
        gcp:
            ip_allocation_policy:
                cluster_ipv4_cidr_block: 10.180.0.0/14
                services_ipv4_cidr_block: 10.8.32.0/20
----

NOTE: Es opcional especificar los bloques CIDR en GKE. Si no se definen, GKE asignará automáticamente un bloque para _pods_ y otro para servicios.

NOTE: Asegúrate de que el bloque CIDR de _pods_ no se solape con el de la VPC ni con otras redes a las que deban acceder los nodos.

== Seguridad

=== Autenticación

Los _controllers_ se comunican con los proveedores _cloud_ utilizando credenciales que se almacenan como _Secrets_ en el _cluster_. Estas credenciales corresponden a la identidad usada durante la instalación y se ubican en el _Namespace_ del _controller_.

==== EKS

En EKS, las credenciales se guardan en un _Secret_ siguiendo el formato estándar de AWS (`~/.aws/credentials`), compatible con la CLI de AWS. La tabla siguiente muestra los _controllers_ implicados, junto con la información relevante de autenticación:

[cols="3,2,1,1,1,1", options="header"]
|===
| Controlador
| ServiceAccount
| Nombre del _Secret_
| Cifrado
| Tipo de autenticación
| Flujo de autenticación

| `capa-controller-manager`
| `capa-controller-manager`
| `capa-manager-bootstrap-credentials`
| Sí (Base64)
| Credenciales de AWS
| OAuth2 Client Credentials

| `capa-controller-manager`
| `capa-controller-manager`
| `capa-webhook-service-cert`
| Sí (Base64)
| kubernetes.io/tls
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-bootstrap-controller-manager`
| `capi-kubeadm-bootstrap-manager`
| `capi-kubeadm-bootstrap-webhook-service-cert`
| Sí (Base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-control-plane-controller-manager`
| `capi-kubeadm-control-plane-manager`
| `capi-kubeadm-control-plane-webhook-service-cert`
| Sí (Base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-controller-manager`
| `capi-manager`
| `capi-webhook-service-cert`
| Sí (Base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `keoscluster-settings`
| Sí (Base64)
| ClientID + Secret (Azure)
| OAuth2 Client Credentials

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `webhook-server-cert`
| Sí (Base64)
| Certificado TLS
| Webhook TLS (ValidatingAdmissionWebhook)
|===

Para ver el contenido de las credenciales:

[source,bash]
----
k -n capa-system get secret capa-manager-bootstrap-credentials -o json | jq -r '.data.credentials' | Base64 -d
----

Resultado esperado:

[source,bash]
----
[default]
aws_access_key_id = XXXXXXXXXXXXXXXXXXXXXXX
aws_secret_access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
region = eu-west-1
----

==== GKE

En GKE, las credenciales se almacenan en un _Secret_ con el formato estándar de configuración de GCP (`~/.gcloud/config`), compatible con la CLI de GCP. La tabla siguiente muestra los _controllers_ implicados, junto con la información relevante de autenticación:

[cols="3,2,1,1,1,1", options="header"]
|===
| Controlador
| ServiceAccount
| Nombre del _Secret_
| Cifrado
| Tipo de autenticación
| Flujo de autenticación

| `capg-controller-manager`
| `capg-manager`
| `capg-manager-bootstrap-credentials`
| Sí (Base64)
| Credenciales de GCP
| OAuth2 Client Credentials

| `capg-controller-manager`
| `capg-manager`
| `capg-webhook-service-cert`
| Sí (Base64)
| kubernetes.io/tls
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-bootstrap-controller-manager`
| `capi-kubeadm-bootstrap-manager`
| `capi-kubeadm-bootstrap-webhook-service-cert`
| Sí (Base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-control-plane-controller-manager`
| `capi-kubeadm-control-plane-manager`
| `capi-kubeadm-control-plane-webhook-service-cert`
| Sí (Base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-controller-manager`
| `capi-manager`
| `capi-webhook-service-cert`
| Sí (Base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `keoscluster-settings`
| Sí (Base64)
| ClientID + Secret (Azure)
| OAuth2 Client Credentials

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `webhook-server-cert`
| Sí (Base64)
| Certificado TLS
| Webhook TLS (ValidatingAdmissionWebhook)
|===

Para ver el contenido de las credenciales:

[source,bash]
----
$ k -n capg-system get secret capg-manager-bootstrap-credentials -o json | jq -r '.data["credentials.json"]' | Base64 -d | jq .
----

==== Azure

En Azure, las credenciales se guardan en un _Secret_ con el formato estándar de configuración (`~/.azure/credentials`), compatible con la CLI de Azure. La tabla siguiente muestra los _controllers_ implicados, junto con la información relevante de autenticación:

[cols="3,2,1,1,1,1", options="header"]
|===
| Controlador
| ServiceAccount
| Nombre del _Secret_
| Cifrado
| Tipo de autenticación
| Flujo de autenticación

| `capz-controller-manager`
| `capz-manager`
| `cluster-identity-secret`
| Sí (Base64)
| ClientID + Secret
| OAuth2 Client Credentials

| `capz-controller-manager`
| `capz-manager`
| `capz-webhook-service-cert`
| Sí (Base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-bootstrap-controller-manager`
| `capi-kubeadm-bootstrap-manager`
| `capi-kubeadm-bootstrap-webhook-service-cert`
| Sí (Base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-control-plane-controller-manager`
| `capi-kubeadm-control-plane-manager`
| `capi-kubeadm-control-plane-webhook-service-cert`
| Sí (Base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-controller-manager`
| `capi-manager`
| `capi-webhook-service-cert`
| Sí (Base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `keoscluster-settings`
| Sí (Base64)
| ClientID + Secret (Azure)
| OAuth2 Client Credentials

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `webhook-server-cert`
| Sí (Base64)
| Certificado TLS
| Webhook TLS (ValidatingAdmissionWebhook)
|===

Para ver el contenido de las credenciales:

[source,bash]
----
k -n capz-system get secret cluster-identity-secret -o json | jq -r '.data["clientSecret"]' | Base64 -d
----

NOTE: Para renovar las credenciales de los _controllers_ (`keoscluster-controller-manager`, `capa`, `capg` o `capz`), consulta la sección xref:operations-manual:credentials.adoc[Renovación de credenciales].

=== Acceso a IMDS

==== EKS (IMDSv2)

Dado que los _pods_ pueden suplantar al nodo accediendo a IMDS, se configura una política de red global de Calico (_GlobalNetworkPolicy_) que restringe el acceso a IMDS para todos los _pods_ que no pertenezcan a _Stratio KEOS_.

Además, se habilita el proveedor OIDC en EKS para permitir el uso de roles IAM con _ServiceAccounts_, aplicando políticas con privilegios mínimos.

Para verificar la configuración de IMDSv2:

[source,bash]
----
# Obtener los ID de todas las instancias asociadas al cluster
INSTANCE_IDS=$(aws ec2 describe-instances \
  --filters "Name=tag:kubernetes.io/cluster/<cluster-name>,Values=owned" \
  --query "Reservations[*].Instances[*].InstanceId" \
  --output text)

# Verificar la configuración de IMDSv2 para cada instancia
for ID in $INSTANCE_IDS; do
  echo "Verificando instancia $ID:"
  aws ec2 describe-instances \
    --instance-ids "$ID" \
    --query "Reservations[*].Instances[*].MetadataOptions" \
    --output json
done
----

=== Acceso al _API Server_

==== EKS

Al crear un _cluster_ de EKS, se genera un _endpoint_ público y otro privado para el _API Server_. Ambos se protegen mediante reglas de IAM y RBAC nativo de Kubernetes.

Para comprobar los _endpoints_ generados, se pueden utilizar los siguientes comandos:

[source,bash]
----
# Obtener la URL del API Server:
aws eks describe-cluster --region <region> --name <cluster_name> --query "cluster.endpoint" --output text | cat
https://XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.gr7.eu-west-1.eks.amazonaws.com
# Verificar el tipo de acceso:
aws eks describe-cluster --region <region>  --name <cluster-name> --query "cluster.resourcesVpcConfig" --output json | cat

    "subnetIds": [
        "subnet-0cd582b2fc8f4667f",
        "subnet-036599062ce4633b4",
        "subnet-0ed8d484e85078953",
        "subnet-0e33205cc1afeb1ae",
        "subnet-01299725d68bc6a10",
        "subnet-0764ad7f79ecee088"
    ],
    "securityGroupIds": [
        "sg-XXXXXXXXXXXXXXXXX"
    ],
    "clusterSecurityGroupId": "sg-XXXXXXXXXXXXXXXXX",
    "vpcId": "vpc-XXXXXXXXXXXXXXXXX",
    "endpointPublicAccess": true,   # Acceso público habilitado
    "endpointPrivateAccess": true,  # Acceso privado habilitado
    "publicAccessCidrs": [
        "0.0.0.0/0"
    ]
}
----

Busca las claves `endpointPublicAccess` y `endpointPrivateAccess` para comprobar si el acceso público y privado están habilitados.

==== GKE

En GKE, el _API Server_ se expone exclusivamente mediante un _endpoint_ privado. Solo se puede acceder a él desde la IP asignada, que debe estar incluida en el rango configurado para el _cluster_.

Para verificar el _endpoint_ privado:

[source,bash]
----
# Obtener IP privada del API Server:
gcloud container clusters describe <cluster_name> --region <region> --format="get(privateClusterConfig.privateEndpoint)"
172.16.16.2
# Verificar que el acceso privado está habilitado:
gcloud container clusters describe <cluster_name> --region <region> --format="get(privateClusterConfig.enablePrivateEndpoint)"
True
----

==== Azure no gestionado

En Azure, el _API Server_ se expone mediante un balanceador de carga público con nombre `<cluster_id>-public-lb`, accesible a través del puerto 6443. La IP pública asignada es la misma que resuelve el _kubeconfig_ del _cluster_, y el _Backend pool_ incluye los nodos del _control-plane_.

El _health check_ predeterminado usa TCP, aunque se recomienda cambiarlo a HTTPS con la ruta `/healthz`.

Para comprobar la exposición del _API Server_:

[source,bash]
----
# Verificar la existencia del balanceador:
az network lb list -g <resource_group> --query "[].{Name:name, PublicIP:frontendIpConfigurations[].publicIpAddress.id}" -o table
Name
----------------
azure-public-lb

# Verificar la IP pública asignada:
az network public-ip list -g <resource_group> \
  --query "[?ipConfiguration.id && contains(ipConfiguration.id, '<load_balancer_name>')].{Name:name, IP:ipAddress}" \
  -o table
Name                  IP
--------------------  -------------
pip-azure-apiserver  132.164.7.182
----

== Almacenamiento

=== Nodos (_control-plane_ y _workers_)

A nivel de almacenamiento, se monta un único disco _root_ del que se puede definir su tipo, tamaño y encriptación (se podrá especificar una clave de encriptación previamente creada).

*Ejemplo:*

[source,bash]
----
type: gp3
size: 384Gi
encrypted: true
encryption_key: <key_name>
----

Estos discos se crean en la provisión inicial de los nodos, por lo que estos datos se pasan como parámetros del descriptor.

=== _StorageClass_

Durante el aprovisionamiento se disponibiliza una _StorageClass_ (por defecto) con nombre "keos" para disco de bloques. Esta cuenta con los parámetros `reclaimPolicy: Delete` y `volumeBindingMode: WaitForFirstConsumer`, esto es, que el disco se creará en el momento en que un _pod_ consuma el _PersistentVolumeClaim_ correspondiente y se eliminará al borrar el _PersistentVolume_.

NOTE: Ten en cuenta que los _PersistentVolumes_ creados a partir de esta _StorageClass_ tendrán afinidad con la zona donde se han consumido.

Desde el descriptor del _cluster_ se permite indicar la clave de encriptación, la clase de discos o bien parámetros libres.

*Ejemplo con opciones básicas:*

[source,bash]
----
spec:
  infra_provider: aws
  storageclass:
    encryption_key: <my_simm_key>
    class: premium
----

El parámetro `class` puede ser _premium_ o _standard_, esto dependerá del proveedor _cloud_:

[.center,cols="1,2,2",width=70%,center]
|===
^|Proveedor ^|Standard class ^|Premium class

^|AWS
^|gp3
^|io2 (64k IOPS)

^|GKE
^|pd-standard
^|pd-ssd

^|Azure
^|StandardSSD_LRS
^|Premium_LRS
|===

*Ejemplo con parámetros libres:*

[source,bash]
----
spec:
  infra_provider: gcp
  storageclass:
    parameters:
      type: pd-extreme
      provisioned-iops-on-create: 5000
      disk-encryption-kms-key: <key_name>
      labels: "key1=value1,key2=value2"
----

Estos últimos también dependen del proveedor _cloud_:

[.center,cols="1,2",width=80%]
|===
^|Proveedor ^|Parámetro

^|All
a|

----
     fsType
----

^|AWS, GKE
a|

----
     type
     labels
----

^|AWS
a|

----
     iopsPerGB
     kmsKeyId
     allowAutoIOPSPerGBIncrease
     iops
     throughput
     encrypted
     blockExpress
     blockSize
----

^|GKE
a|

----
     provisioned-iops-on-create
     replication-type
     disk-encryption-kms-key
----

^|Azure
a|

----
     provisioner
     skuName
     kind
     cachingMode
     diskEncryptionType
     diskEncryptionSetID
     resourceGroup
     tags
     networkAccessPolicy
     publicNetworkAccess
     diskAccessID
     enableBursting
     enablePerformancePlus
     subscriptionID
----

|===

En el aprovisionamiento se crean otras _StorageClasses_ (no default) según el proveedor, pero para utilizarlas, las cargas de trabajo deberán especificarlas en su despliegue.

== Atributos en EKS

Todos los objetos que se crean en EKS contienen por defecto el atributo con clave _keos.stratio.com/owner_ y como valor el nombre del _cluster_. También se permite añadir atributos personalizados a todos los objetos creados en el proveedor _cloud_ de la siguiente forma:

[source,bash]
----
spec:
  control_plane:
    tags:
      - tier: production
      - billing-area: data
----

Para añadir atributos a los volúmenes creados por la _StorageClass_, se deberá utilizar el parámetro `labels` en la sección correspondiente:

[source,bash]
----
spec:
  storageclass:
    parameters:
      labels: "tier=production,billing-area=data"
      ..
----

== Docker _registries_

Antes de instalar _Stratio KEOS_, todas las imágenes Docker deben estar disponibles en un _registry_ que se indique en `spec.docker_registries.docker_registries.url` del descriptor del _cluster_, y debe incluir la opción `keos_registry: true`.

Debe haber uno (y solo uno) Docker _registry_ principal para _Stratio KEOS_. Los demás se configuran en los nodos para permitir el despliegue de otras imágenes.

Se admiten 4 tipos de _registries_: `acr`, `ecr`, `gar` y `generic`.

* Para `generic`, es obligatorio indicar si requiere autenticación. En caso afirmativo, se deben proporcionar usuario y contraseña en 'spec.credentials'.
* Los tipos `acr`, `ecr` y `gar` no permiten autenticación manual.

La siguiente tabla muestra los _registries_ compatibles por _proveedor_:

[.center,cols="2,1",width=40%]
|===
^|EKS
^|ecr, generic

^|Azure
^|acr, generic

^|GKE
^|gar
|===

== Repositorio de Helm

Como prerrequisito de la instalación, se debe indicar un repositorio de Helm del que se pueda extraer el _chart_ del _Cluster Operator_. Este repositorio puede utilizar protocolos HTTPS u OCI (utilizados para repositorios de proveedores _cloud_ como ECR, GAR o ACR).

[.center,cols="2,1",width=40%]
|===
^|EKS
^|ecr, generic

^|Azure
^|acr, generic

^|GKE
^|gar
|===

NOTE: Las URL de los repositorios de tipo OCI llevan el prefijo *oci://*. Por ejemplo: oci://stratioregistry.azurecr.io/helm-repository-example.

NOTE: Recuerda verificar en la documentación de _keos-installer_ los repositorios que se soporten en la versión a utilizar.
