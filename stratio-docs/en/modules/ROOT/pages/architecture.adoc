= Architecture

Reference architecture:

image::eks-reference-architecture.png[]

== Provisioning phases

image::arq-intro.png[]

== Introduction

_Stratio Cloud Provisioner_ is the first phase in creating a _Stratio KEOS_ cluster in a cloud provider. This phase includes provisioning infrastructure (virtual machines, networks, load balancers, etc.), making the Kubernetes cluster, its network, and storage—all within the selected cloud provider.

During this phase, a Kubernetes resource called _KeosCluster_ is generated, which defines the characteristics of the cluster and serves as the single point for managing its lifecycle. Based on the provided cluster descriptor, two files are also created: a descriptor file (_keos.yaml_) and another with encrypted credentials (_secrets.yml_). Both will be used in the next phase: the installation of _Stratio KEOS_.

Once the cluster is installed, all cloud infrastructure maintenance and management tasks (day 2 operations) must be performed exclusively by editing the _KeosCluster_ resource. This ensures that the _Stratio Cluster Operator _ controls administration.

== _KeosCluster_ resource

When the Helm chart for the _Stratio Cluster Operator_ is deployed and a cluster descriptor is provided, a _KeosCluster_ resource is automatically created. This resource centralizes both the creation and management of the cluster and cloud-specific resources.

=== Concurrency control and error prevention

To avoid conflicts and reduce human error, the _Stratio Cluster Operator_ blocks new operations if one is already in progress on the _KeosCluster_ resource. This status is reflected in the `status` subresource, which indicates the active operation type.

== _ClusterConfig_ resource

Along with the _KeosCluster_ resource, another resource called _ClusterConfig_ can be defined to set additional configuration for the cluster.

This resource must be declared in the same file as the _KeosCluster_ resource. If not specified during installation, it will be automatically generated with default values.

TIP: For more information, see the xref:operations-manual:api-reference.adoc[API reference] section.

=== Selecting the _Cluster Operator_

By default, the latest available version of the chart from the Helm repository specified in the _KeosCluster_ resource will be installed. This behavior can be modified by manually selecting the desired version in the _ClusterConfig_ resource configuration.

TIP: For more information, see the xref:operations-manual:api-reference.adoc[API reference] section.

==== Version selection criteria

Automatic version selection is based on the following criteria:

- Version type priority: _release_, _prerelease_, _milestone_, _snapshot_, _pull request_.
- Alphanumeric order within the same version type (except _prerelease_).

For example, version `0.2.1` is prioritized over `0.2.0`. In the case of _prerelease_ versions without a clear order, the latest published in the Helm repository is selected, following Stratio’s naming scheme.

NOTE: If the default selection is used, there is no need to specify the version manually. It will be reflected in the resource created within the cluster.

== Cloud provider-generated resources

In a *default deployment*, the following resources are created for each cloud provider. The specific resources depend on the contents of the cluster descriptor.

=== EKS

[cols="2,1", options="header"]
|===
| Resource
| Quantity

| Amazon EKS cluster
| 1

| EKS-managed _add-ons_ (EBS CSI Driver, CoreDNS, kube-proxy, VPC CNI)
| 1 set

| Logging _add-ons_ (CloudWatch)
| 1 (if specified)

| OIDC Provider
| 1

| VPC
| 1

| Public subnets (one per availability zone)
| 3

| Private subnets (one per availability zone)
| 3

| Internet Gateway
| 1

| NAT Gateways (one per public subnet)
| 3

| Route tables with Internet Gateway egress
| 3

| Route tables with NAT Gateway egress
| 3

| Security groups (for _control-plane_ and _Worker_ nodes)
| 2

| IAM Role `<cluster-name>-iam-service-role` for the _control-plane_
| 1

| IAM Policy `AmazonEKSClusterPolicy` attached to the role above
| 1

| IAM Role for cluster nodes `nodes.cluster-api-provider-aws.sigs.k8s.io`
| 1

| IAM Policy for the nodes `nodes.cluster-api-provider-aws.sigs.k8s.io`
| 1

| Virtual machines for _Worker_ nodes
| Based on the descriptor and autoscaling

| Persistent EBS volumes
| 1 per requested volume

| Network Load Balancer
| 1

| Listeners (one per port of each _Service_)
| Variable
|===

[IMPORTANT]
.Client permission requirements
====
Depending on the cloud account's permissions, the following roles and policies may need to be pre-created. Otherwise, they will be automatically created during the process:

* Role: `nodes.cluster-api-provider-aws.sigs.k8s.io`
* Policy: `nodes.cluster-api-provider-aws.sigs.k8s.io`, attached to the role above
* Role: `<cluster-name>-iam-service-role (controlplane)`
* Policy: `AmazonEKSClusterPolicy` (already exists in Amazon), attached to the role above
** xref:attachment$nodes-cluster-api-provider-aws-sigs-k8s-io.json[Download EKS node policy]
** xref:attachment$nodes-trust-relationship.json[Download EKS node trust relationship policy]
====

=== GKE (private cluster)

[cols="2,1", options="header"]
|===
| Resource
| Quantity

| Google Kubernetes Engine (GKE) cluster with native VPC networking
| 1

| VPC
| 1

| Subnet per region
| 1

| Primary CIDR block (for nodes)
| 1

| Secondary CIDR block (for pods and services)
| 1 per type

| Peering route (VPC Network Peering)
| 1

| Routes for secondary CIDR blocks (pods and services)
| 2

| VPC _peering_ network
| 1

| VPC firewall rules
(gke-<cluster-name>-<id>-[master, vms, exkubelet, inkubelet, all])
| 5

| Virtual machines for _Worker_ nodes
| Based on the descriptor and autoscaling

| Persistent volumes
| 1 per node
|===

=== Unmanaged Azure

[cols="2,1", options="header"]
|===
| Resource
| Quantity

| Resource Group
| 1

| Virtual Network
| 1

| Route table for _Worker_ nodes
| 1

| NAT Gateway for _Worker_ nodes
| 1

| Public IP addresses (API Server and NAT Gateway)
| 2

| Network Security Groups (NSGs) for _control-plane_ and _workers_
| 2

| Public Load Balancer for the API Server
| 1

| Virtual machines for the _control-plane_
| 1–3 (based on descriptor)

| Block disk per _control-plane_ VM
| 1 per VM

| Network interface per _control-plane_ VM
| 1 per VM

| Virtual machines for _Worker_ nodes
| Based on the descriptor and autoscaling

| Block disk per _Worker_ VM
| 1 per VM

| Network interface per _Worker_ VM
| 1 per VM

| Load Balancer to expose _LoadBalancer_-type _Services_
| 1

| Public IP address per exposed _Service_
| 1 per _Service_

| Frontend IP configuration per _Service_
| 1 per _Service_

| Health probe per _Service_
| 1 per _Service_

| Load balancer rule per _Service_
| 1 per _Service_

| Block disk for persistent volumes
| 1 per requested volume
|===

== Networking

Reference architecture

image::eks-reference-architecture.png[]

The internal networking layer of the cluster is based on Calico, with the following integrations per provider:

[.center,cols="1,1,1,1,1,1",center]
|===
^|Provider ^|Policy ^|IPAM ^|CNI ^|Overlay ^|Routing

^|*EKS*
^|Calico
^|AWS
^|AWS
^|No
^|VPC-native

^|*GKE*
^|Calico
^|Calico
^|Calico
^|No
^|VPC-nativa

^|*Azure*
^|Calico
^|Calico
^|Calico
^|VxLAN
^|Calico
|===

=== Proprietary infrastructure

Although one of the advantages of automatic resource creation in provisioning is the great dynamism it provides, for security and compliance reasons, it is often necessary to create certain resources before the deployment of _Stratio KEOS_ in the cloud provider.

In this context, the _Stratio Cloud Provisioner_ allows the use of both pre-created VPCs and subnets by using the `networks` parameter in the cluster descriptor, as detailed in the xref:operations-manual:installation.adoc[installation guide].

Example for EKS:

[source,bash]
----
spec:
  networks:
    vpc_id: vpc-02698....
    subnets:
      - subnet_id: subnet-0416d...
      - subnet_id: subnet-0b2f8...
      - subnet_id: subnet-0df75...
----

=== Pods network

In most providers, it's possible to specify a custom CIDR block for pods, with certain specifics described below.

NOTE: The CIDR for pods must not overlap with the nodes' network or any other target network that the nodes need to access.

==== EKS

In this case, and since the AWS VPC CNI is used as IPAM, only one of the two ranges supported by EKS will be allowed: 100.64.0.0.0/16 or 198.19.0.0.0/16 (always taking into account the restrictions of the https://docs.aws.amazon.com/vpc/latest/userguide/vpc-cidr-blocks.html#add-cidr-block-restrictions[official documentation]), which will be added to the VPC as secondary CIDR.

NOTE: If no custom infrastructure is indicated, the CIDR 100.64.0.0.0/16 should be used.

[source,bash]
----
spec:
  networks:
	  pods_cidr: 100.64.0.0/16
----

In this case, 3 subnets will be created (1 per zone) with an 18-bit mask (/18) of the indicated range from which the IPs for the pods will be obtained:

[.center,cols="1,2",width=40%, options="header"]
|===
^|*Zone*
^|*CIDR*

^|zone-a
^|100.64.0.0/18

^||zone-b
^|100.64.64.0/18

^||zone-c
^|100.64.128.0/18
|===

NOTE: The secondary CIDR assigned to the VPC for pods must be specified using the `spec.networks.pods_cidr` parameter.

In the case of using custom infrastructure, the 3 subnets (one per zone) for the pods must be indicated together with those of the nodes in the cluster descriptor:

[source,bash]
----
spec:
  networks:
      vpc_id: vpc-0264503b4f41ff69f # example-custom-vpc
      pods_subnets:
          - subnet_id: subnet-0f6aa193eaa31015e # example-custom-sn-pods-zone-a
          - subnet_id: subnet-0ad0a80d1cec762d7 # example-custom-sn-pods-zone-b
          - subnet_id: subnet-0921f337cb6a6128d # example-custom-sn-pods-zone-c
      subnets:
          - subnet_id: subnet-0416da6767f910929 # example-custom-sn-priv-zone-a
          - subnet_id: subnet-0b2f81b89da1dfdfd # example-custom-sn-priv-zone-b
          - subnet_id: subnet-0df75719efe5f6615 # example-custom-sn-priv-zone-c
----

The CIDR of each subnet (derived from the VPC's secondary CIDR) must match the one described above (with an 18-bit mask), and the 3 subnets for pods must include the following tag: _sigs.k8s.io/cluster-api-provider-aws/association=secondary_.

==== Unmanaged Azure

In this provider/flavor, Calico is used as the IPAM for the CNI. This allows the definition of an arbitrary CIDR block for the pods network, as shown in the example below:

[source,bash]
----
spec:
  networks:
	  pods_cidr: 100.64.0.0/10
----

NOTE: It is recommended to use one of the following blocks for `pods_cidr`, as long as they do not overlap with the cluster's VNet: `100.64.0.0/10`, `172.16.0.0/12`, or `192.168.0.0/16`. The block `100.64.0.0/10` is especially recommended, as it does not fall within RFC1918 ranges and reduces the risk of conflicts.

==== GKE

In GKE, the pods network is automatically configured from the secondary CIDR of the VPC network defined during cluster creation.

If you need to set a pods network manually, you can do so in two mutually exclusive ways:

* Pre-create the CIDR ranges in the VPC subnet and reference them in the cluster descriptor:
+
[source,bash]
----
spec:
  control_plane:
          managed: true
          gcp:
              ip_allocation_policy:
                  cluster_secondary_range_name: "gkepods-europe-west4"
                  services_secondary_range_name: "gkeservices-europe-west4"
----

* Directly define the CIDR blocks in the cluster descriptor and let GKE create them automatically:
+
[source,bash]
----
spec:
  control_plane:
        managed: true
        gcp:
            ip_allocation_policy:
                cluster_ipv4_cidr_block: 10.180.0.0/14
                services_ipv4_cidr_block: 10.8.32.0/20
----

NOTE: Defining CIDR blocks in GKE is optional. If not specified, GKE will automatically assign one block for pods and another for services.

NOTE: Ensure that the pods CIDR block does not overlap with the VPC or other networks that the nodes need to access.

== Security

=== Authentication

Controllers communicate with cloud providers using credentials stored as _Secrets_ in the cluster. These credentials correspond to the identity used during installation and are located in the controller’s namespace.

==== EKS

In EKS, credentials are stored in a _Secret_ following AWS’s standard format (`~/.aws/credentials`), compatible with the AWS CLI. The following table shows the involved controllers along with the relevant authentication information:

[cols="3,2,1,1,1,1", options="header"]
|===
| Controller
| ServiceAccount
| Secret Name
| Encrypted
| Auth Type
| Auth Flow

| `capa-controller-manager`
| `capa-controller-manager`
| `capa-manager-bootstrap-credentials`
| Yes (Base64)
| GCP credentials
| OAuth2 Client Credentials

| `capa-controller-manager`
| `capa-controller-manager`
| `capa-webhook-service-cert`
| Yes (Base64)
| kubernetes.io/tls
| Webhook TLS (Mutating/Validating admission)

| `capi-kubeadm-bootstrap-controller-manager`
| `capi-kubeadm-bootstrap-manager`
| `capi-kubeadm-bootstrap-webhook-service-cert`
| Yes (Base64)
| TLS certificate
| Webhook TLS (Mutating/Validating admission)

| `capi-kubeadm-control-plane-controller-manager`
| `capi-kubeadm-control-plane-manager`
| `capi-kubeadm-control-plane-webhook-service-cert`
| Yes (Base64)
| TLS certificate
| Webhook TLS (Mutating/Validating admission)

| `capi-controller-manager`
| `capi-manager`
| `capi-webhook-service-cert`
| Yes (Base64)
| TLS certificate
| Webhook TLS (Mutating/Validating admission)

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `keoscluster-settings`
| Yes (Base64)
| ClientID + Secret (Azure)
| OAuth2 Client Credentials

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `webhook-server-cert`
| Yes (Base64)
| TLS certificate
| Webhook TLS (ValidatingAdmissionWebhook)
|===

To inspect credential contents:

[source,bash]
----
k -n capa-system get secret capa-manager-bootstrap-credentials -o json | jq -r '.data.credentials' | Base64 -d
----

Expected output:

[source,bash]
----
[default]
aws_access_key_id = XXXXXXXXXXXXXXXXXXXXXXX
aws_secret_access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
region = eu-west-1
----

==== GKE

In GKE, credentials are stored in a _Secret_ following the standard GCP configuration format (`~/.gcloud/config`), compatible with the GCP CLI. The following table shows the relevant controllers and authentication information:

[cols="3,2,1,1,1,1", options="header"]
|===
| Controller
| ServiceAccount
| Secret Name
| Encrypted
| Auth Type
| Auth Flow

| `capg-controller-manager`
| `capg-manager`
| `capg-manager-bootstrap-credentials`
| Yes (Base64)
| GCP credentials
| OAuth2 Client Credentials

| `capg-controller-manager`
| `capg-manager`
| `capg-webhook-service-cert`
| Yes (Base64)
| kubernetes.io/tls
| Webhook TLS (Mutating/Validating admission)

| `capi-kubeadm-bootstrap-controller-manager`
| `capi-kubeadm-bootstrap-manager`
| `capi-kubeadm-bootstrap-webhook-service-cert`
| Yes (Base64)
| TLS certificate
| Webhook TLS (Mutating/Validating admission)

| `capi-kubeadm-control-plane-controller-manager`
| `capi-kubeadm-control-plane-manager`
| `capi-kubeadm-control-plane-webhook-service-cert`
| Yes (Base64)
| TLS certificate
| Webhook TLS (Mutating/Validating admission)

| `capi-controller-manager`
| `capi-manager`
| `capi-webhook-service-cert`
| Yes (Base64)
| TLS certificate
| Webhook TLS (Mutating/Validating admission)

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `keoscluster-settings`
| Yes (Base64)
| ClientID + Secret (Azure)
| OAuth2 Client Credentials

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `webhook-server-cert`
| Yes (Base64)
| TLS certificate
| Webhook TLS (ValidatingAdmissionWebhook)
|===

To inspect credential contents:

[source,bash]
----
$ k -n capg-system get secret capg-manager-bootstrap-credentials -o json | jq -r '.data["credentials.json"]' | Base64 -d | jq .
----

==== Azure

In Azure, credentials are stored in a _Secret_ using the standard configuration format (`~/.azure/credentials`), compatible with the Azure CLI. The following table shows the relevant controllers and authentication information:

[cols="3,2,1,1,1,1", options="header"]
|===
| Controller
| ServiceAccount
| Secret Name
| Encrypted
| Auth Type
| Auth Flow

| `capz-controller-manager`
| `capz-manager`
| `cluster-identity-secret`
| Yes (Base64)
| ClientID + Secret
| OAuth2 Client Credentials

| `capz-controller-manager`
| `capz-manager`
| `capz-webhook-service-cert`
| Yes (Base64)
| TLS certificate
| Webhook TLS (Mutating/Validating admission)

| `capi-kubeadm-bootstrap-controller-manager`
| `capi-kubeadm-bootstrap-manager`
| `capi-kubeadm-bootstrap-webhook-service-cert`
| Yes (Base64)
| TLS certificate
| Webhook TLS (Mutating/Validating Admission))

| `capi-kubeadm-control-plane-controller-manager`
| `capi-kubeadm-control-plane-manager`
| `capi-kubeadm-control-plane-webhook-service-cert`
| Yes (Base64)
| TLS certificate
| Webhook TLS (Mutating/Validating Admission)

| `capi-controller-manager`
| `capi-manager`
| `capi-webhook-service-cert`
| Yes (Base64)
| TLS certificate
| Webhook TLS (Mutating/Validating Admission)

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `keoscluster-settings`
| Yes (Base64)
| ClientID + Secret (Azure)
| OAuth2 Client Credentials

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `webhook-server-cert`
| Yes (Base64)
| TLS certificate
| Webhook TLS (ValidatingAdmissionWebhook)
|===

To inspect credential contents:

[source,bash]
----
k -n capz-system get secret cluster-identity-secret -o json | jq -r '.data["clientSecret"]' | base64 -d
----

NOTE: To renew controller credentials (`keoscluster-controller-manager`, `capa`, `capg`, or `capz`), refer to the xref:operations-manual:credentials.adoc[Credential Renewal] section.

=== Access to IMDS

==== EKS (IMDSv2)

Since pods can impersonate the node by accessing IMDS, a global Calico network policy (_GlobalNetworkPolicy_) is configured to restrict IMDS access to all pods except those belonging to _Stratio KEOS_.

Additionally, the OIDC provider is enabled in EKS to allow the use of IAM roles with ServiceAccounts, enforcing least privilege policies.

To verify IMDSv2 configuration:

[source,bash]
----
# Get all instance IDs associated with the cluster
INSTANCE_IDS=$(aws ec2 describe-instances \
  --filters "Name=tag:kubernetes.io/cluster/<cluster-name>,Values=owned" \
  --query "Reservations[*].Instances[*].InstanceId" \
  --output text)

# Verify IMDSv2 configuration for each instance
for ID in $INSTANCE_IDS; do
  echo "Verificando instancia $ID:"
  aws ec2 describe-instances \
    --instance-ids "$ID" \
    --query "Reservations[*].Instances[*].MetadataOptions" \
    --output json
done
----

=== API Server access

==== EKS

When creating an EKS cluster, both a public and a private API Server endpoint are generated. Both are secured using IAM rules and Kubernetes’ native RBAC.

To check the generated endpoints, you can run the following commands:

[source,bash]
----
# Get the API Server URL:
aws eks describe-cluster --region <region> --name <cluster_name> --query "cluster.endpoint" --output text | cat
https://XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.gr7.eu-west-1.eks.amazonaws.com
# Check access type:
aws eks describe-cluster --region <region>  --name <cluster-name> --query "cluster.resourcesVpcConfig" --output json | cat

    "subnetIds": [
        "subnet-0cd582b2fc8f4667f",
        "subnet-036599062ce4633b4",
        "subnet-0ed8d484e85078953",
        "subnet-0e33205cc1afeb1ae",
        "subnet-01299725d68bc6a10",
        "subnet-0764ad7f79ecee088"
    ],
    "securityGroupIds": [
        "sg-XXXXXXXXXXXXXXXXX"
    ],
    "clusterSecurityGroupId": "sg-XXXXXXXXXXXXXXXXX",
    "vpcId": "vpc-XXXXXXXXXXXXXXXXX",
    "endpointPublicAccess": true,   # Acceso público habilitado
    "endpointPrivateAccess": true,  # Acceso privado habilitado
    "publicAccessCidrs": [
        "0.0.0.0/0"
    ]
}
----

Check the `endpointPublicAccess` and `endpointPrivateAccess` keys to verify whether public and private access are enabled.

==== GKE

In GKE, the API Server is exposed exclusively via a private endpoint. It is only accessible from the assigned IP address, which must be within the configured cluster range.

To verify the private endpoint:

[source,bash]
----
# Get the API Server's private IP:
gcloud container clusters describe <cluster_name> --region <region> --format="get(privateClusterConfig.privateEndpoint)"
172.16.16.2
# Check that private access is enabled:
gcloud container clusters describe <cluster_name> --region <region> --format="get(privateClusterConfig.enablePrivateEndpoint)"
True
----

==== Unmanaged Azure

In Azure, the API Server is exposed via a public load balancer named `<cluster_id>-public-lb`, accessible through port 6443. The assigned public IP is the same one used in the cluster _kubeconfig_, and the backend pool includes the _control-plane_ nodes.

The default health check uses TCP, although switching to HTTPS with the `/healthz` path is recommended.

To check API Server exposure:

[source,bash]
----
# Check for the existence of the load balancer:
az network lb list -g <resource_group> --query "[].{Name:name, PublicIP:frontendIpConfigurations[].publicIpAddress.id}" -o table
Name
----------------
azure-public-lb

# Check the assigned public IP:
az network public-ip list -g <resource_group> \
  --query "[?ipConfiguration.id && contains(ipConfiguration.id, '<load_balancer_name>')].{Name:name, IP:ipAddress}" \
  -o table
Name                  IP
--------------------  -------------
pip-azure-apiserver  132.164.7.182
----

== Storage

=== Nodes (_control-plane_ and _workers_)

Regarding storage, a single root disk is mounted and its type, size and encryption can be defined (you can specify a previously created encryption key).

Example:

[source,bash]
----
type: gp3
size: 384Gi
encrypted: true
encryption_key: <key_name>
----

These disks are created in the initial provisioning of the _worker_ nodes, so this data is passed as descriptor parameters.

=== _StorageClass_

By default, a _StorageClass_ with the name "keos" is made available for block disk during provisioning. This _StorageClass_ is created with the parameters `reclaimPolicy: Delete` and `volumeBindingMode: WaitForFirstConsumer`, i.e. the disk will be created at the moment a pod consumes the corresponding _PersistentVolumeClaim_ and will be deleted when the _PersistentVolume_ is deleted.

NOTE: Note that _PersistentVolumes_ created from this _StorageClass_ will have an affinity to the area where they have been consumed.

From the cluster descriptor it is possible to indicate the encryption key, the class of disks or free parameters.

*Example with basic options:*

[source,bash]
----
spec:
  infra_provider: aws
  storageclass:
    encryption_key: <my_simm_key>
    class: premium
----

The `class` parameter can be _premium_ or _standard_, this will depend on the cloud provider:

[.center,cols=“1,2,2”,width=70%,center]
|===
^|Provider ^|Standard class ^|Premium class

^|AWS
^|gp3
^|io2 (64k IOPS)

^|GKE
^|pd-standard
^|pd-ssd

^|Azure
^|StandardSSD_LRS
^|Premium_LRS
|===

*Example with free parameters:*

[source,bash]
----
spec:
  infra_provider: gcp
  storageclass:
    parameters:
      type: pd-extreme
      provisioned-iops-on-create: 5000
      disk-encryption-kms-key: <nombre_clave>
      tags: "key1=value1,key2=value2"
----

The latter also depend on the cloud provider:

[.center,cols="1,2",width=80%]
|===
^|Provider ^|Parameter

^|All
a|

----
     fsType
----

^|AWS, GKE
a|

----
     type
     labels
----

^|AWS
a|

----
     iopsPerGB
     kmsKeyId
     allowAutoIOPSPerGBIncrease
     iops
     throughput
     encrypted
     blockExpress
     blockSize
----

^|GKE
a|

----
     provisioned-iops-on-create
     replication-type
     disk-encryption-kms-key
----

^|Azure
a|

----
     provisioner
     skuName
     kind
     cachingMode
     diskEncryptionType
     diskEncryptionSetID
     resourceGroup
     tags
     networkAccessPolicy
     publicNetworkAccess
     diskAccessID
     enableBursting
     enablePerformancePlus
     subscriptionID
----

|===

Other non-default _StorageClasses_ are created in provisioning depending on the provider, but to use them workloads will need to specify them in their deployment.

== Tags in EKS

All objects created in EKS contain by default the tag with key _keos.stratio.com/owner_ and as a value the name of the cluster. It is also allowed to add custom tags to all objects created in the cloud provider as follows:

[source,bash]
----
spec:
  control_plane:
    tags:
      - tier: production
      - billing-area: data
----

To add attributes to the volumes created by the _StorageClass_, use the `labels` parameter in the corresponding section:

[source,bash]
----
spec:
  storageclass:
    parameters:
      labels: "tier=production,billing-area=data"
      ..
----

== Docker registries

Before installing _Stratio KEOS_, all Docker images must be available in a registry specified in the `spec.docker_registries.docker_registries.url` field of the cluster descriptor, and it must include the option `keos_registry: true`.

There must be one (and only one) primary Docker registry for _Stratio KEOS_. Additional registries may be configured on the nodes to allow deploying other images.

Four registry types are supported: `acr`, `ecr`, `gar`, and `generic`.

* For `generic`, you must specify whether authentication is required. If so, a username and password must be provided in 'spec.credentials'.
* The types `acr`, `ecr`, and `gar` do not allow manual authentication.

The following table shows which registry types are supported per provider:

[.center,cols="2,1",width=40%]
|===
^|EKS
^|ecr, generic

^|Azure
^|acr, generic

^|GKE
^|gar
|===

== Helm repository

As a prerequisite of the installation, a Helm repository must be specified from which the _Cluster Operator_ chart can be extracted. This repository can use HTTPS or OCI protocols (used for cloud provider repositories such as ECR, GAR or ACR).

[.center,cols="2,1",width=40%]
|===
^|EKS
^|ecr, generic

^|Azure
^|acr, generic

^|GKE
^|gar
|===

NOTE: URLs for OCI repositories are prefixed with *oci://*. For example, oci://stratioregistry.azurecr.io/helm-repository-example.

NOTE: Remember to check the _keos-installer_ documentation for the repositories supported in the version to be used.
