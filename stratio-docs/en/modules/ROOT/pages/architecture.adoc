= Architecture

Reference architecture

image::eks-reference-architecture.png[]

== Introduction

image::arq-intro.png[]

_Stratio Cloud Provisioner_ is the initial phase for the creation of a _Stratio KEOS_ cluster in a cloud provider. It comprises the provisioning of the infrastructure (virtual machines, private network, load balancers, etc. in the cloud), the creation of a Kubernetes cluster, its networking and storage.

Upon completion of the cluster creation in this phase and according to an indicated cluster descriptor, a descriptor file (_keos.yaml_) and another encrypted credentials file (_secrets.yml_) will be created for the next _Stratio KEOS_ installation phase.

== Cloud provider objects

In a *default deployment*, the following objects are created in each cloud provider (in [silver]#gray# optional objects that will depend on what is specified in the cluster descriptor):

=== EKS

* 1 Elastic Kubernetes Service (EKS) cluster with add-ons for EBS and CNI, logging (if specified) and an OIDC provider.
** 2 EKS Security Groups for the _control-plane_ and the _worker_ nodes.
** 1 IAM role with the _AmazonEKSClusterPolicy_.
* [silver]#1 VPC.#
* [silver]#6 subnets with their respective routing tables.#
** [silver]#3 public subnets (one per AZ).#
** [silver]#3 private subnets (also one per AZ).#
* [silver]#1 NAT gateway for each public subnet.#
* [silver]#1 Internet gateway for the VPC.#
* [silver]#1 default route in the routing table of each private subnet to go out to the Internet through the NAT gateways.#
* [silver]#1 default route in the routing table of each public subnet to exit the internet through the internet gateway.#
* 1 IAM policy for the nodes of the cluster (_nodes.cluster-api-provider-aws.sigs.k8s.io_).
* 1 IAM role for the nodes of the cluster (_nodes.cluster-api-provider-aws.sigs.k8s.k8s.io_).
* VMs for _Workers_ (according to cluster descriptor and auto-scaling).
** 1 EBS volume per persistent volume.
* 1 _Network_ type load balancer for service exposure.
** 1 _Listener_ per port for each _Service_ type load balancer.
* EBS volume for each persistent volume.

=== GCP

* 1 SSL/TCP load balancer for the API Server.
* 1 Health check for the _Unmanage Instance Group_.
* 1 CloudNat VPC partner.
* 1 Cloud Router.
* Firewall rules.
* 1 _Unmanage Instance Group_ for the _control-plane_.
* 1/3 VMs for the _control-plane_ (according to the cluster descriptor).
** 1 Persistent disk per VM.
* VMs for workers (according to the cluster descriptor and auto-scaling).
** 1 Persistent disk per VM.
* 1 L4 load balancer for service exposure.
** 1 _Listener_ per port for each service of load balancer type.
* Persistent disk for each persistent volume.

=== Azure unmanaged

* [silver]#1 Resource group.#
* 1 virtual network.
* 1 Route table for _workers_.
* 1 NAT gateway for _workers_.
* 2 public IP address (API Server and NATgw for _workers_).
* 2 Network Security Group (_control-plane_ and _workers_).
* 1 public load balancer.
* 1/3 VMs for the _control-plane_ (according to the cluster descriptor).
** 1 block disk per VM.
** 1 network interface per VM.
* VMs for _workers_ (according to the cluster descriptor and autoscaling).
** 1 block disk per VM.
** 1 network interface per VM.
* 1 load balancer for the exposure of _Services_ type Load Balancer.
** 1 public IP address for each _service_.
** 1 _Frontend IP config_ for each _service_.
** 1 _Health probe_ for each _service_.
** 1 load balancer rule for each _service_.
* Block disk for each persistent volume.

=== AKS

* 1 Azure Kubernetes Service (AKS) cluster.
* 2 Resource groups (for AKS and _workers_).
* 2 Virtual Network (for AKS and _workers_).
* 1 public IP address (for _workers_ output).
* 1 Network Security Group for _workers_.
* 1 Managed Identity.
* VM Scale Sets for _workers_ (according to the descriptor of the cluster).
* 1 load balancer for the exposure of _Services_ type Load Balancer.
** 1 public IP address for each _service_.
** 1 _Frontend IP config_ for each _service_.
** 1 _Health probe_ for each _service_.
** 1 load balancer rule for each _service_.
* Block disk for each persistent volume.

== Networking

Reference architecture

image::eks-reference-architecture.png[]

The internal networking layer of the cluster is based on Calico, with the following integrations per provider/flavour:

[.center,cols=“1,1,1,1,1,1”,center]
|===
^|Provider/flavour ^|Policy ^|IPAM ^|CNI ^|Overlay ^|Routing

^|EKS
^|Calico
^|AWS
^|AWS
^|No
^|VPC-native

^|GCP
^|Calico
^|Calico
^|Calico
^|IpIp
^|BGP

^|Azure
^|Calico
^|Calico
^|Calico
^|VxLAN
^||Calico

^|AKS
^|Calico
^|Azure
^|Azure
^|No
^|VPC-native
|===

=== Proprietary infrastructure

Although one of the advantages of the automatic creation of resources in provisioning is the great dynamism it provides, for security and compliance reasons, it is often necessary to create certain resources prior to the deployment of _Stratio KEOS_ in the cloud provider.

In this sense, _Stratio Cloud Provisioner_ allows using both a VPC and subnets previously created using the `networks` parameter in the cluster descriptor, as detailed in the xref:ROOT:installation.adoc[installation guide].

An example for EKS is shown below:

[source,bash]
----
spec:
  networks:
    vpc_id: vpc-02698..
    subnets:
      - subnet_id: subnet-0416d..
      - subnet_id: subnet-0b2f8..
      - subnet_id: subnet-0df75..
----

=== Pods network

CAUTION: In *deployments with AKS* the pods CIDR configuration is currently not supported, since the cloud provider's IPAM is used.

In most providers/flavours it is allowed to indicate a specific CIDR for pods, with certain particularities described below.

NOTE: The CIDR for pods must not overlap with the nodes' network or any other destination network that the nodes need to access.

==== EKS

In this case, and since the AWS VPC CNI is used as IPAM, only one of the two ranges supported by EKS will be allowed: 100.64.0.0/16 or 198.19.0.0/16 (always taking into account the restrictions of the https://docs.aws.amazon.com/vpc/latest/userguide/vpc-cidr-blocks.html#add-cidr-block-restrictions[official documentation]), which will be added to the VPC as secondary CIDR.

NOTE: If no custom infrastructure is specified, the CIDR 100.64.0.0.0/16 should be used.

[source,bash]
----
spec:
  networks:
	  pods_cidr: 100.64.0.0/16
----

3 subnets will be created (1 per zone) with an 18-bit mask (/18) of the indicated range, from which the IPs for the pods will be obtained:

[.center,cols=“1,2”,width=40%]
|===
^|zone-a
^|100.64.0.0/18

^||zone-b
^|100.64.64.0/18

^||zone-c
^|100.64.128.0/18
|===

In case of using a customized infrastructure, the 3 subnets (one per zone) for the pods must be indicated together with those of the nodes in the cluster descriptor:

[source,bash]
----
spec:
  networks:
      vpc_id: vpc-0264503b4f41ff69f # example-custom-vpc
      pods_subnets:
          - subnet_id: subnet-0f6aa193eaa31015e # example-custom-sn-sn-_pods_-zone-a
          - subnet_id: subnet-0ad0a80d1cec762d7 # example-custom-sn-_pods_-zone-b
          - subnet_id: subnet-0921f337cb6a6128d # example-custom-sn-_pods_-zone-c
      subnets:
          - subnet_id: subnet-0416da676767f910929 # example-custom-sn-priv-zone-a
          - subnet_id: subnet-0b2f81b89da1dfdfd # example-custom-sn-priv-zone-b
          - subnet_id: subnet-0df75719efe5f6615 # example-custom-sn-priv-zone-c
      pods_cidr: 100.64.0.0.0/16
----

NOTE: The secondary CIDR assigned to the VPC for the pods must be specified in the `spec.networks.pods_cidr` parameter.

The CIDR of each subnet (obtained from the secondary CIDR of the VPC) must be the same as described above (with 18-bit mask) and the 3 subnets for pods must have the following tag: `sigs.k8s.io/cluster-api-provider-aws/association=secondary`.

==== GCP and unmanaged Azure

In these providers/flavours Calico is used as the IPAM of the CNI, which allows to be able to specify an arbitrary CIDR for the pods:

[source,bash]
----
spec:
  [source,bash] networks:
	  pods_cidr: 172.16.0.0/20
----

== Security

=== Authentication

Currently, for communication with cloud providers, the _controllers_ store in the cluster the credentials of the identity used in the installation. Such credentials can be viewed with the following commands.

==== AWS

For this provider, the credentials are stored in a _Secret_ within the namespace of the _controller_ with the format of the ~/.aws/credentials file:

[source,bash]
----
k -n layer-system get secret layer-manager-bootstrap-credentials -o json | jq -r '.data.credentials' | base64 -d
----

==== GCP

As for EKS, the GCP _controller_ gets credentials from a _Secret_ within the corresponding namespace.

[source,bash]
----
$ k -n capg-system get secret capg-manager-bootstrap-credentials -o json | jq -r '.data[“credentials.json”]' | base64 -d | jq .
----

==== Azure

For Azure, the _client++_++_++id_ is stored in the _AzureIdentity_ object within the namespace of the _controller_, which also has the reference to the _Secret_ where the _client++_++secret_ is stored:

*_client++_++_++id_*.
+
[source,bash]
----
$ k -n capz-system get azureidentity -o json | jq -r .items[0].spec.clientID
----

*_client++_++secret_*
+
[source,bash]
----
$ CLIENT_PASS_NAME=$$(k -n capz-system get azureidentity -o json | jq -r .items[0].spec.clientPassword.name)
$ CLIENT_PASS_NAMESPACE=$$(k -n capz-system get azureidentity -o json | jq -r .items[0].spec.clientPassword._Namespace_)
$ kubectl -n ${CLIENT_PASS_NAMESPACE} get secret ${CLIENT_PASS_NAME} -o json | jq -r .data.clientSecret | base64 -d; echo
----

=== IMDS access (for EKS and GCP)

Since pods can impersonate the node where they run by simply interacting with IMDS, a global network policy (Calico's _GlobalNetworkPolicy_) is used to prevent access to all pods in the cluster that are not part of _Stratio KEOS_.

In turn, in EKS the OIDC provider is enabled to allow the use of IAM roles for Service Accounts, ensuring the use of least privilege IAM policies.

=== Access to the API Server endpoint

==== EKS

During the creation of the EKS cluster, an endpoint is created for the API Server to be used for access to the cluster from the installer and lifecycle operations.

This endpoint is published to the internet, and its access is restricted with a combination of AWS Identity and Access Management (IAM) rules and Kubernetes' native Role Based Access Control (RBAC).

==== GCP

For API Server exposure, a load balancer is created with name `<cluster_id>-apiserver` and port 443 accessible by public network (the assigned public IP is the same as configured in the _Kubeconfig_) and one _instance groups_ per AZ (1 or 3, depending on HA configuration) with the corresponding _control-plane_ node.

The health check of the service is done via SSL, but it is recommended to switch to HTTPS with the `/healthz` path.

==== Azure unmanaged

For the API Server exposure, a load balancer is created with name `<cluster_id>-public-lb` and port 6443 accessible by public network (the assigned public IP is the same that resolves the _Kubeconfig_ URL) and a _Backend pool_ with the _control-plane_ nodes.

The health check of the service is done by TCP, but it is recommended to switch to HTTPS with the `/healthz` route.

==== AKS

In this case, the API Server is exposed publicly and with the URL indicated in the _kubeconfig_.

== Storage

=== Nodes (_control-plane_ and _workers_)

At the storage level, a single root disk is mounted of which you can define its type, size and encryption (you can specify a previously created encryption key).

*Example:*

[source,bash]
----
type: gp3
size: 384Gi
encrypted: true
encryption_key: <key_name>
----

These disks are created in the initial provisioning of the nodes, so the data is passed as descriptor parameters.

=== _StorageClass_

During provisioning, a _StorageClass_ with name "keos" is available by default for block disk. This _StorageClass_ has the parameters `reclaimPolicy: Delete` and `volumeBindingMode: WaitForFirstConsumer`, that is, the disk will be created the moment a pod consumes the corresponding _PersistentVolumeClaim_ and will be deleted when the _PersistentVolume_ is deleted.

NOTE: Note that _PersistentVolumes_ created from the _StorageClass_ will have affinity to the zone where they have been consumed.

From the cluster descriptor it is allowed to indicate the encryption key, the disk class or free parameters.

*Example with basic options:*

[source,bash]
----
spec:
  infra_provider: aws
  storageclass:
    encryption_key: <mi_clave_sim>
    clase: premium
----

The `class` parameter can be "premium" or "standard", this will depend on the cloud provider:

[.center,cols=“1,2,2”,width=70%,center]
|===
^|Provider ^|Standard class ^|Premium class

^|AWS
^|gp3
^|io2 (64k IOPS)

^|GCP
^|pd-standard
^|pd-ssd

^|Azure
^|StandardSSD_LRS
^|Premium_LRS
|===

*Example with free parameters:*

[source,bash]
----
spec:
  infra_provider: gcp
  storageclass:
    parameters:
      type: pd-extreme
      provisioned-iops-on-create: 5000
      disk-encryption-kms-key: <key_name>
      labels: “key1=value1,key2=value2”
----

The latter also depend on the cloud provider:

[.center,cols=“1,2”,width=80%]
|===
^|Provider ^|Parameter

^|All
a|
----
     fsType
----

^|AWS, GCP
a|
----
     type
     labels
----

^|AWS
a|
----
     iopsPerGB
     kmsKeyId
     allowAutoIOPSPerGBIncrease
     iops
     throughput
     encrypted
     blockExpress
     blockSize
----

^|GCP
a|
----
     provisioned-iops-on-create
     replication-type
     disk-encryption-kms-key
----

^|Azure
a|
----
     provisioner
     skuName
     kind
     cachingMode
     diskEncryptionType
     diskEncryptionSetID
     resourceGroup
     tags
     networkAccessPolicy
     publicNetworkAccess
     diskAccessID
     enableBursting
     enablePerformancePlus
     subscriptionID
----

|===

Other _StorageClasses_ are created in provisioning (not by default) depending on the vendor, but to use them workloads will need to specify them in their deployment.

=== Amazon EFS

If this version wishes to use an EFS file system, it must be created beforehand and the following data must be passed to the cluster descriptor:

[source,bash]
----
spec:
  storageclass:
      efs:
          name: fs-015ea5ea5e2ba5fe7fa5
          id: fs-015ea5ea5e2ba5fe7fa5
          permissions: 640
----

With this data the _keos.yaml_ will be rendered, so that in the execution of the _keos-installer_ the driver is displayed and the corresponding _StorageClass_ is configured.

NOTE: This functionality is intended for custom infrastructure, since the EFS file system must be associated to an existing VPC at its creation.

== Tags in EKS

All objects created in EKS contain by default the tag with key _keos.stratio.com/owner_ and as value the cluster name. It is also allowed to add custom tags to all objects created in the cloud provider as follows:

[source,bash]
----
spec:
  control_plane:
    tags:
      - tier: production
      - billing-area: data
----

To add tags to the volumes created by the _StorageClass_, the `labels` parameter must be used in the corresponding section:

[source,bash]
----
spec:
  storageclass:
    parameters:
      “tier=production,billing-area=data” labels.
      ..
----

== Docker registries

As a prerequisite for the installation of _Stratio KEOS_, the Docker images of all its components must reside in a Docker registry to be indicated in the cluster descriptor (`keos_registry: true`). There should be one (and only one) Docker registry for _Stratio KEOS_, the rest will be configured on the nodes to be able to use their images in any deployment.

Currently 3 types of Docker registries are supported: _generic_, _ecr_ and _acr_. For the _generic_ type you must indicate if the registry is authenticated or not (_ecr_ and _acr_ types cannot have authentication), and in case it is authenticated, it is mandatory to indicate user and password in the 'spec.credentials' section.

The following is a table of supported registries according to the provider/flavour:

[.center,cols=“2,1”,width=40%]
|===
^|EKS
^|ecr, generic

^|GCP
^|generic

^|Azure
^|acr, generic

^|AKS
^|acr
|===
