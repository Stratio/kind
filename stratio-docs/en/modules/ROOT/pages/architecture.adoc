= Architecture

Reference architecture:

image::eks-reference-architecture.png[]

== Introduction

image::arq-intro.png[]

_Stratio Cloud Provisioner_ es la fase inicial para la creación de un _cluster_ de _Stratio KEOS_ en un proveedor _cloud_. Esta fase comprende el aprovisionamiento de la infraestructura (máquinas virtuales, redes privadas, balanceadores de carga, etc.), la creación de un _cluster_ de Kubernetes, su _networking_ y almacenamiento, todo ello en el proveedor seleccionado.

Durante esta fase, se genera un recurso de Kubernetes denominado _KeosCluster_, el cual define las características del _cluster_ y actúa como punto único de gestión de su ciclo de vida. A partir del descriptor de _cluster_ proporcionado, también se generan dos archivos: un descriptor (`keos.yaml`) y otro cifrado con las credenciales (`secrets.yml`) los cuales serán utilizados en la siguiente fase de instalación de _Stratio KEOS_.

Una vez completada la instalación, todas las operaciones de mantenimiento y gestión de la infraestructura _cloud_ (día 2) deberán realizarse exclusivamente mediante la edición del recurso _KeosCluster_, garantizando así una administración controlada por el _Stratio Cluster Operator_.

== _KeosCluster_ object

Tras desplegar el _chart_ de Helm del _Stratio Cluster Operator_ y con base en el descriptor del _cluster_, se crea automáticamente un recurso _KeosCluster_. Este recurso centraliza la creación y administración del _cluster_ y los objetos específicos del proveedor _cloud_.

=== Control de concurrencia y prevención de errores

Para evitar conflictos y mitigar errores humanos, el _Stratio Cluster Operator_ deniega nuevas solicitudes si ya existe una operación en curso sobre el objeto _KeosCluster_. Esta situación se refleja mediante su subrecurso `status`, que informa del tipo de operación activa.

== _ClusterConfig_ object

In the same way as the _KeosCluster_ object, during the installation phase, the _ClusterConfig_ resource will be created to indicate specific configurations for that particular cluster.

The declaration of this Kubernetes resource must be made in the descriptor file itself, next to the _KeosCluster_ object. If it is not indicated during the cluster installation, the resource will be generated with the default values.

TIP: For more details, see the xref:operations-manual:api-reference.adoc[API reference] section.

=== Choice of the _Cluster Operator_

By default, the latest version of the chart available in the Helm repository indicated in the _KeosCluster_ object will be installed, although this behavior can be overridden and you can indicate the version you want to install by indicating the value in the xref:operations-manual:api-reference.adoc[_ClusterConfig_ configuration].

The choice of the latest available version considers both the order of precedence of the versioning and the alphanumeric ordering of the versions. That is, priority will be given to versions in the following order: _release_, _prerelease_, _milestone_, _snapshot_ and _pull++_++request_.

In case several versions of the same precedence (except _prerreleases_) coincide in the same repository, the last alphanumerically ordered version will be returned. Thus, for example, version 0.2.1 will be returned before version 0.2.0.

There is an exception to this choice mechanism. Due to the versioning naming in Stratio, if the existing versions with the highest precedence are _prerreleases_, the last version uploaded to the Helm repository will be returned because the order cannot be set alphanumerically.

NOTE: In the case of choosing the default chart, it will not be necessary to indicate the field during the installation but this field will be reflected when the object is created in the cluster with the version that has been retrieved.

== Cloud provider objects

En un *despliegue por defecto* se crean los siguientes objetos en cada proveedor _cloud_ (los objetos generados dependerán de lo especificado en el descriptor del _cluster_):

=== EKS

[cols="2,1", options="header"]
|===
| Recurso
| Cantidad

| Cluster de Elastic Kubernetes Service (EKS)
| 1

| Add-ons gestionados por EKS (EBS CSI Driver, CoreDNS, kube-proxy, VPC CNI)
| 1 conjunto

| Add-on de logging (CloudWatch)
| 1 (si se especifica)

| Proveedor OIDC
| 1

| VPC*
| 1

| Subnets públicas (una por zona de disponibilidad)
| 3

| Subnets privadas (una por zona de disponibilidad)
| 3

| Internet Gateway*
| 1

| NAT Gateways (uno por subnet pública)
| 3

| Tablas de rutas con salida a Internet Gateway
| 3

| Tablas de rutas con salida a NAT Gateway
| 3

| Security Groups (control-plane y nodos _Worker_)
| 2

| IAM Role `<cluster-name>-iam-service-role` para el _control-plane_
| 1

| Asociar IAM Policy `AmazonEKSClusterPolicy` al rol `<cluster-name>-iam-service-role`
| 1

| IAM Role para _nodos del cluster_ `nodes.cluster-api-provider-aws.sigs.k8s.io`
| 1

| IAM Policy para nodos (`nodes.cluster-api-provider-aws.sigs.k8s.io`)
| 1

| Máquinas virtuales para nodos _Worker_
| Según descriptor y autoescalado

| Volúmenes EBS persistentes
| 1 por volumen solicitado

| Balanceador de carga tipo _Network_
| 1

| Listeners (uno por puerto de cada _Service_)
| Variable
|===

[IMPORTANT]
.Dependencias según permisos del cliente
====
Dependiendo de los permisos disponibles en la cuenta del cliente, será necesario crear previamente los siguientes roles y políticas como prerrequisitos. 

En caso contrario, estos se crearán automáticamente durante el proceso:

* Rol: `nodes.cluster-api-provider-aws.sigs.k8s.io`
* Política: `nodes.cluster-api-provider-aws.sigs.k8s.io` y asociarla al rol `nodes.cluster-api-provider-aws.sigs.k8s.io`
* Rol: `<cluster-name>-iam-service-role (controlplane)`
* Política: `AmazonEKSClusterPolicy` (ya creada por Amazon) y asociarla al rol `<cluster-name>-iam-service-role`
+
** xref:attachment$nodes-cluster-api-provider-aws-sigs-k8s-io.json[Descargar política de nodos EKS]
** xref:attachment$nodes-trust-relationship.json[Descargar política de relación confianza de nodos EKS]
====

=== GKE (_cluster_ privado)

[cols="2,1", options="header"]
|===
| Recurso
| Cantidad

| Cluster de Google Kubernetes Engine (GKE) configurado con VPC-nativa
| 1

| VPC
| 1

| Subred por región
| 1

| Bloque CIDR principal para subred (nodos)
| 1

| Bloque CIDR secundario para subred (pods y servicios)
| 1 por tipo

| Ruta de peering (VPC Network Peering)
| 1

| Rutas para bloques CIDR secundarios (pods y servicios)
| 2

| Red de VPC peering
| 1

| Reglas de firewall de VPC  
(gke-<nombre-cluster>-<id>-[master, vms, exkubelet, inkubelet, all])
| 5

| Máquinas virtuales para nodos _Worker_
| Según descriptor y autoescalado

| Volúmenes persistentes
| 1 por nodo
|===

=== Azure no gestionado

[cols="2,1", options="header"]
|===
| Recurso
| Cantidad

| Resource Group
| 1

| Red virtual (Virtual Network)
| 1

| Route table para nodos _Worker_
| 1

| NAT Gateway para nodos _Worker_
| 1

| Direcciones IP públicas (API Server y NAT Gateway)
| 2

| Grupos de seguridad de red (NSG) para _control-plane_ y _workers_
| 2

| Balanceador de carga público para API Server
| 1

| Máquinas virtuales para _control-plane_
| 1–3 (según descriptor)

| Disco de bloque por máquina virtual de _control-plane_
| 1 por VM

| Interfaz de red por máquina virtual de _control-plane_
| 1 por VM

| Máquinas virtuales para nodos _Worker_
| Según descriptor y autoescalado

| Disco de bloque por máquina virtual de _Worker_
| 1 por VM

| Interfaz de red por máquina virtual de _Worker_
| 1 por VM

| Balanceador de carga para exposición de _Services_ tipo LoadBalancer
| 1

| Dirección IP pública por _Service_ expuesto
| 1 por Service

| Configuración de IP frontal (_Frontend IP config_) por _Service_
| 1 por Service

| _Health probe_ por _Service_
| 1 por Service

| Regla de balanceador de carga por _Service_
| 1 por Service

| Disco de bloque para volúmenes persistentes
| 1 por volumen solicitado
|===

== Networking

Reference architecture

image::eks-reference-architecture.png[]

The internal networking layer of the cluster is based on Calico, with the following integrations per provider/flavor:

[.center,cols="1,1,1,1,1,1",center]
|===
^|Provider ^|Policy ^|IPAM ^|CNI ^|Overlay ^|Routing

^|*EKS*
^|Calico
^|AWS
^|AWS
^|No
^|VPC-native

^|*GKE*
^|Calico
^|Calico
^|Calico
^|No
^|VPC-nativa

^|*Azure*
^|Calico
^|Calico
^|Calico
^|VxLAN
^|Calico
|===

=== Proprietary infrastructure

Although one of the advantages of automatic resource creation in provisioning is the great dynamism it provides, for security and compliance reasons, it is often necessary to create certain resources before the deployment of _Stratio KEOS_ in the cloud provider.

In this sense, the _Stratio Cloud Provisioner_ allows using both a VPC and subnets previously created using the networks parameter in the cluster descriptor, as detailed in the xref:operations-manual:installation.adoc[installation guide].

Example for EKS:

[source,bash]
----
spec:
  networks:
    vpc_id: vpc-02698....
    subnets:
      - subnet_id: subnet-0416d...
      - subnet_id: subnet-0b2f8...
      - subnet_id: subnet-0df75...
----

=== Pods network

In most providers it is allowed to specify a specific CIDR for pods, with certain particularities described below.

NOTE: The CIDR for pods must not overlap with the nodes' network or any other target network that the nodes need to access.

==== EKS

In this case, and since the AWS VPC CNI is used as IPAM, only one of the two ranges supported by EKS will be allowed: 100.64.0.0.0/16 or 198.19.0.0.0/16 (always taking into account the restrictions of the https://docs.aws.amazon.com/vpc/latest/userguide/vpc-cidr-blocks.html#add-cidr-block-restrictions[official documentation]), which will be added to the VPC as secondary CIDR.

NOTE: If no custom infrastructure is indicated, the CIDR 100.64.0.0.0/16 should be used.

[source,bash]
----
spec:
  networks:
	  pods_cidr: 100.64.0.0/16
----

In this case, 3 subnets will be created (1 per zone) with an 18-bit mask (/18) of the indicated range from which the IPs for the pods will be obtained:

[.center,cols="1,2",width=40%, options="header"]
|===
^|**Zone**
^|**CIDR**

^|zone-a
^|100.64.0.0/18

^||zone-b
^|100.64.64.0/18

^||zone-c
^|100.64.128.0/18
|===

NOTE: El CIDR secundario asignado al VPC para los _pods_ debe indicarse en el parámetro `spec.networks.pods_cidr` obligatoriamente.

In the case of using custom infrastructure, the 3 subnets (one per zone) for the pods must be indicated together with those of the nodes in the cluster descriptor:

[source,bash]
----
spec:
  networks:
      vpc_id: vpc-0264503b4f41ff69f # example-custom-vpc
      pods_subnets:
          - subnet_id: subnet-0f6aa193eaa31015e # example-custom-sn-pods-zone-a
          - subnet_id: subnet-0ad0a80d1cec762d7 # example-custom-sn-pods-zone-b
          - subnet_id: subnet-0921f337cb6a6128d # example-custom-sn-pods-zone-c
      subnets:
          - subnet_id: subnet-0416da6767f910929 # example-custom-sn-priv-zone-a
          - subnet_id: subnet-0b2f81b89da1dfdfd # example-custom-sn-priv-zone-b
          - subnet_id: subnet-0df75719efe5f6615 # example-custom-sn-priv-zone-c
      pods_cidr: 100.64.0.0.0/16
----

NOTE: The secondary CIDR assigned to the VPC for the pods must be indicated in the `spec.networks.pods_cidr` parameter.

The CIDR of each subnet (obtained from the secondary CIDR of the VPC), must be the same as described above (with 18-bit mask), and the 3 subnets for pods must have the following tag: _sigs.k8s.io/cluster-api-provider-aws/association=secondary_.

==== Azure unmanaged

In this provider/flavor Calico is used as the IPAM of the CNI, this allows to be able to specification of an arbitrary CIDR for the pods:

[source,bash]
----
spec:
  [source,bash] networks:
	  pods_cidr: 100.64.0.0/10
----

NOTE: Nuestra recomendación es usar bloques 100.64.0.0/10, 172.16.0.0/12, o 192.168.0.0/16 para pods_cidr si no se solapan con la VNet del _cluster_.
El rango 100.64.0.0/10 es altamente recomendado para evitar conflictos con RFC1918.

==== GKE

The pods network in GKE is automatically configured with the secondary CIDR for pods and services, obtained from the VPC network configuration when the cluster is deployed.

Para especificar una red de _pods_ diferente, podremos hacerlo de 2 formas excluyentes entre sí:

* Pre-creando los rangos CIDR en la subnet de la red VPC y especificando el CIDR en el descriptor del _cluster_.

[source,bash]
----
spec:
  control_plane:
          managed: true
          gcp:
              ip_allocation_policy:
                  cluster_secondary_range_name: "gkepods-europe-west4"
                  services_secondary_range_name: "gkeservices-europe-west4"
----

* Indicando el CIDR en el descriptor del _cluster_ y dejando que GKE lo cree automáticamente.

[source,bash]
----
spec:
  control_plane:
        managed: true
        gcp:
            ip_allocation_policy:
                cluster_ipv4_cidr_block: 10.180.0.0/14
                services_ipv4_cidr_block: 10.8.32.0/20
----

NOTE: La elección de un bloque CIDR para _pods_ y servicios en GKE es opcional, ya que si no se especifica, GKE asignará automáticamente un bloque CIDR a la red de _pods_ y otro a la de servicios.

NOTE: El rango de _pods_ no debe solaparse con el bloque CIDR de la red VPC o cualquier otra red a la que los nodos deban acceder.

== Security

=== Authentication

Currently, for communication with cloud providers, the controllers store in the cluster the credentials of the identity used in the installation.

These credentials can be viewed with the following commands:

==== EKS

Para este proveedor, las credenciales se almacenan en un _Secret_ dentro del _Namespace_ del _controller_ (capa-system/capi-system/kube-system) utilizando el formato estándar de configuración de credenciales de AWS (`~/.aws/credentials`), que sigue la especificación de perfiles de AWS CLI.

A continuación se muestra una tabla con los controladores utilizados y la ubicación de sus credenciales:

[cols="3,2,1,1,1,1", options="header"]
|===
| Controlador
| ServiceAccount
| Nombre del Secret
| Cifrado
| Tipo de autenticación
| Flujo de autenticación

| `capa-controller-manager`
| `capa-controller-manager`
| `capa-manager-bootstrap-credentials`
| Sí (base64)
| Credenciales de AWS
| OAuth2 Client Credentials

| `capa-controller-manager`
| `capa-controller-manager`
| `capa-webhook-service-cert`
| Sí (base64)
| kubernetes.io/tls
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-bootstrap-controller-manager`
| `capi-kubeadm-bootstrap-manager`
| `capi-kubeadm-bootstrap-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-control-plane-controller-manager`
| `capi-kubeadm-control-plane-manager`
| `capi-kubeadm-control-plane-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-controller-manager`
| `capi-manager`
| `capi-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `keoscluster-settings`
| Sí (base64)
| ClientID + Secret (Azure)
| OAuth2 Client Credentials

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `webhook-server-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (ValidatingAdmissionWebhook)

|===

Para ver el contenido de las credenciales, se puede utilizar el siguiente comando a modo de ejemplo:

[source,bash]
----
k -n capa-system get secret capa-manager-bootstrap-credentials -o json | jq -r '.data.credentials' | base64 -d

[default]
aws_access_key_id = XXXXXXXXXXXXXXXXXXXXXXX
aws_secret_access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
region = eu-west-1
----

==== GKE

Los _controllers_ de GKE almacenan las credenciales en un _Secret_ dentro del _Namespace_ del _controller_ (capg-system/capi-system/kube-system) utilizando el formato estándar de configuración de credenciales de GCP (`~/.gcloud/config`), que sigue la especificación de perfiles de GCP.

A continuación se muestra una tabla con los controladores utilizados y la ubicación de sus credenciales:

[cols="3,2,1,1,1,1", options="header"]
|===
| Controlador
| ServiceAccount
| Nombre del Secret
| Cifrado
| Tipo de autenticación
| Flujo de autenticación

| `capg-controller-manager`
| `capg-manager`
| `capg-manager-bootstrap-credentials`
| Sí (base64)
| Credenciales de GCP
| OAuth2 Client Credentials

| `capg-controller-manager`
| `capg-manager`
| `capg-webhook-service-cert`
| Sí (base64)
| kubernetes.io/tls
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-bootstrap-controller-manager`
| `capi-kubeadm-bootstrap-manager`
| `capi-kubeadm-bootstrap-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-control-plane-controller-manager`
| `capi-kubeadm-control-plane-manager`
| `capi-kubeadm-control-plane-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-controller-manager`
| `capi-manager`
| `capi-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `keoscluster-settings`
| Sí (base64)
| ClientID + Secret (Azure)
| OAuth2 Client Credentials

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `webhook-server-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (ValidatingAdmissionWebhook)
|===

[source,bash]
----
$ k -n capg-system get secret capg-manager-bootstrap-credentials -o json | jq -r '.data["credentials.json"]' | base64 -d | jq .
----

==== Azure

Los _controllers_ de Azure almacenan las credenciales en un _Secret_ dentro del _Namespace_ del _controller_ (capz-system/capi-system/kube-system) utilizando el formato estándar de configuración de credenciales de Azure (`~/.azure/credentials`), que sigue la especificación de perfiles de Azure.

[cols="3,2,1,1,1,1", options="header"]
|===
| Controlador
| ServiceAccount
| Nombre del Secret
| Cifrado
| Tipo de autenticación
| Flujo de autenticación

| `capz-controller-manager`
| `capz-manager`
| `cluster-identity-secret`
| Sí (base64)
| ClientID + Secret
| OAuth2 Client Credentials

| `capz-controller-manager`
| `capz-manager`
| `capz-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-bootstrap-controller-manager`
| `capi-kubeadm-bootstrap-manager`
| `capi-kubeadm-bootstrap-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-kubeadm-control-plane-controller-manager`
| `capi-kubeadm-control-plane-manager`
| `capi-kubeadm-control-plane-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `capi-controller-manager`
| `capi-manager`
| `capi-webhook-service-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (Mutating/Validating Admission)

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `keoscluster-settings`
| Sí (base64)
| ClientID + Secret (Azure)
| OAuth2 Client Credentials

| `keoscluster-controller-manager`
| `keoscluster-controller-manager`
| `webhook-server-cert`
| Sí (base64)
| Certificado TLS
| Webhook TLS (ValidatingAdmissionWebhook)
|===

Para ver el contenido de las credenciales, se puede utilizar el siguiente comando a modo de ejemplo:

[source,bash]
----
k -n capz-system get secret cluster-identity-secret -o json | jq -r '.data["clientSecret"]' | base64 -d
----

NOTE: Para actualizar las credenciales del _keoscluster-controller-manager_ o de los controladores _capa_, _capg_ o _capz_, consulta la sección correspondiente en la guía de xref:operations-manual:credentials.adoc[Renovación de credenciales].

=== IMDS access

Since pods can impersonate the node where they run by simply interacting with IMDS, a global network policy (Calico's _GlobalNetworkPolicy_) is used to prevent access to all pods in the cluster that are not part of _Stratio KEOS_.

In turn, the EKS OIDC provider is enabled to allow the use of IAM roles for _Service Accounts_, ensuring the use of the IAM policies with minimal privileges.

Para verificar la configuración de IMDSv2, se puede utilizar el siguiente comando:

[source,bash]
----
# Obtener los IDs de todas las instancias asociadas al cluster
INSTANCE_IDS=$(aws ec2 describe-instances \
  --filters "Name=tag:kubernetes.io/cluster/<cluster-name>,Values=owned" \
  --query "Reservations[*].Instances[*].InstanceId" \
  --output text)

# Verificar la configuración de IMDSv2 para cada instancia
for ID in $INSTANCE_IDS; do
  echo "Verificando instancia $ID:"
  aws ec2 describe-instances \
    --instance-ids "$ID" \
    --query "Reservations[*].Instances[*].MetadataOptions" \
    --output json
done
----

=== Access to the API Server endpoint

==== EKS

During the creation of the EKS cluster, an endpoint is created for the API Server to be used for accessing the cluster from the installer and lifecycle operations.

This endpoint is published to the internet, and its access is restricted with a combination of Amazon's Identity and Access Management (IAM) rules, and Kubernetes' native Role Based Access Control (RBAC).

Para comprobar la creación y el tipo de acceso del _endpoint_, se pueden utilizar los siguientes comandos:

[source,bash]
----
# Comprobar la creaación del _endpoint_:
aws eks describe-cluster --region <region> --name <cluster_name> --query "cluster.endpoint" --output text | cat 
https://XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.gr7.eu-west-1.eks.amazonaws.com
# Comprobar el tipo de acceso:
aws eks describe-cluster --region <region>  --name <cluster-name> --query "cluster.resourcesVpcConfig" --output json | cat

    "subnetIds": [
        "subnet-0cd582b2fc8f4667f",
        "subnet-036599062ce4633b4",
        "subnet-0ed8d484e85078953",
        "subnet-0e33205cc1afeb1ae",
        "subnet-01299725d68bc6a10",
        "subnet-0764ad7f79ecee088"
    ],
    "securityGroupIds": [
        "sg-XXXXXXXXXXXXXXXXX"
    ],
    "clusterSecurityGroupId": "sg-XXXXXXXXXXXXXXXXX",
    "vpcId": "vpc-XXXXXXXXXXXXXXXXX",
    "endpointPublicAccess": true,   # Acceso público habilitado
    "endpointPrivateAccess": true,  # Acceso privado habilitado
    "publicAccessCidrs": [
        "0.0.0.0/0"
    ]
}

----

==== GKE

In this case, the _API Server_ is exposed only privately, so it can only be accessed from the IP assigned to the cluster's private endpoint. This IP belongs to the range specified in the cluster descriptor.

Para comprobar la creación y el tipo de acceso del _endpoint_, se pueden utilizar los siguientes comandos:
[source,bash]
----
# Comprobar la creación del _endpoint_:
gcloud container clusters describe <cluster_name> --region <region> --format="get(privateClusterConfig.privateEndpoint)"
172.16.16.2
# Comprobar el tipo de acceso:
gcloud container clusters describe <cluster_name> --region <region> --format="get(privateClusterConfig.enablePrivateEndpoint)"
True
----

==== Azure unmanaged

For the API Server exposure, a load balancer is created with the name `<cluster_id>-public-lb` and port 6443 accessible by the public network (the assigned public IP is the same that resolves the _Kubeconfig_ URL) and a Backend pool with the _control-plane_ nodes.

The health check of the service is done over TCP, but it is recommended to change to HTTPS with the `/healthz` path.

Para validar la exposición del API Server en Azure, se pueden utilizar los siguientes comandos:

[source,bash]
----
# Comprobar la creación del _endpoint_:
az network lb list -g <resource_group> --query "[].{Name:name, PublicIP:frontendIpConfigurations[].publicIpAddress.id}" -o table
Name
----------------
azure-public-lb
# Comprobar la ip de exposición:
az network public-ip list -g <resource_group> \
  --query "[?ipConfiguration.id && contains(ipConfiguration.id, '<load_balancer_name>')].{Name:name, IP:ipAddress}" \
  -o table
Name                  IP
--------------------  -------------
pip-azure-apiserver  132.164.7.182
----

== Storage

=== Nodes (_control-plane_ and _workers_)

Regarding storage, a single root disk is mounted and its type, size and encryption can be defined (you can specify a previously created encryption key).

Example:

[source,bash]
----
type: gp3
size: 384Gi
encrypted: true
encryption_key: <key_name>
----

These disks are created in the initial provisioning of the _worker_ nodes, so this data is passed as descriptor parameters.

=== _StorageClass_

By default, a _StorageClass_ with the name "keos" is made available for block disk during provisioning. This _StorageClass_ is created with the parameters `reclaimPolicy: Delete` and `volumeBindingMode: WaitForFirstConsumer`, i.e. the disk will be created at the moment a pod consumes the corresponding _PersistentVolumeClaim_ and will be deleted when the _PersistentVolume_ is deleted.

NOTE: Note that _PersistentVolumes_ created from this _StorageClass_ will have an affinity to the area where they have been consumed.

From the cluster descriptor it is possible to indicate the encryption key, the class of disks or free parameters.

*Example with basic options:*

[source,bash]
----
spec:
  infra_provider: aws
  storageclass:
    encryption_key: <my_simm_key>
    class: premium
----

The `class` parameter can be _premium_ or _standard_, this will depend on the cloud provider:

[.center,cols=“1,2,2”,width=70%,center]
|===
^|Provider ^|Standard class ^|Premium class

^|AWS
^|gp3
^|io2 (64k IOPS)

^|GKE
^|pd-standard
^|pd-ssd

^|Azure
^|StandardSSD_LRS
^|Premium_LRS
|===

*Example with free parameters:*

[source,bash]
----
spec:
  infra_provider: gcp
  storageclass:
    parameters:
      type: pd-extreme
      provisioned-iops-on-create: 5000
      disk-encryption-kms-key: <nombre_clave>
      tags: "key1=value1,key2=value2"
----

The latter also depend on the cloud provider:

[.center,cols="1,2",width=80%]
|===
^|Provider ^|Parameter

^|All
a|

----
     fsType
----

^|AWS, GKE
a|

----
     type
     labels
----

^|AWS
a|

----
     iopsPerGB
     kmsKeyId
     allowAutoIOPSPerGBIncrease
     iops
     throughput
     encrypted
     blockExpress
     blockSize
----

^|GKE
a|

----
     provisioned-iops-on-create
     replication-type
     disk-encryption-kms-key
----

^|Azure
a|

----
     provisioner
     skuName
     kind
     cachingMode
     diskEncryptionType
     diskEncryptionSetID
     resourceGroup
     tags
     networkAccessPolicy
     publicNetworkAccess
     diskAccessID
     enableBursting
     enablePerformancePlus
     subscriptionID
----

|===

Other non-default _StorageClasses_ are created in provisioning depending on the provider, but to use them workloads will need to specify them in their deployment.

== Tags in EKS

All objects created in EKS contain by default the tag with key _keos.stratio.com/owner_ and as a value the name of the cluster. It is also allowed to add custom tags to all objects created in the cloud provider as follows:

[source,bash]
----
spec:
  control_plane:
    tags:
      - tier: production
      - billing-area: data
----

To add attributes to the volumes created by the _StorageClass_, use the `labels` parameter in the corresponding section:

[source,bash]
----
spec:
  storageclass:
    parameters:
      labels: "tier=production,billing-area=data"
      ..
----

== Docker registries

Como prerrequisito a la instalación de _Stratio KEOS_, las imágenes Docker de todos sus componentes deberán residir en un Docker registry que se indicará en el descriptor del _cluster_ en 'spec.docker_registries.docker_registries.url' y deberá llevar la opción (`keos_registry: true`). Deberá haber un (y sólo uno) Docker registry para _Stratio KEOS_, el resto se configurarán en los nodos para poder utilizar sus imágenes en cualquier despliegue.

Actualmente, se soportan 3 tipos de Docker registries: _acr_, _ecr_, _gar_ y _generic_. Para el tipo _generic_, se deberá indicar si el _registry_ es autenticado o no (los tipos _ecr_, _acr_ y _gar_ no pueden tener autenticación), y en caso de serlo, es obligatorio indicar usuario y contraseña en la sección 'spec.credentials'.

La siguiente tabla muestra los _registries_ soportados según _proveedor_:

[.center,cols="2,1",width=40%]
|===
^|EKS
^|ecr, generic

^|Azure
^|acr, generic

^|GKE
^|gar
|===

== Helm repository

As a prerequisite of the installation, a Helm repository must be specified from which the _Cluster Operator_ chart can be extracted. This repository can use HTTPS or OCI protocols (used for cloud provider repositories such as ECR, GAR or ACR).

[.center,cols="2,1",width=40%]
|===
^|EKS
^|ecr, generic

^|Azure
^|acr, generic

^|GKE
^|gar
|===

NOTE: URLs for OCI repositories are prefixed with *oci://*. For example, oci://stratioregistry.azurecr.io/helm-repository-example.

NOTE: Remember to check the _keos-installer_ documentation for the repositories supported in the version to be used.
