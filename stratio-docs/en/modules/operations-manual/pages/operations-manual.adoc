= Operations manual

== Obtaining the _kubeconfig_

To communicate with the API Server of the created cluster, the _kubeconfig_ file is necessary, which can be obtained differently depending on the cloud provider used and the _control-plane_ management of the cluster.

* For EKS, it will be obtained as indicated by AWS:
+
[source,bash]
----
aws eks update-kubeconfig --region eu-west-1 --name <cluster_name> --kubeconfig ./<cluster_name>.kubeconfig
----

* For GKE, the credentials will be obtained as specified by GCP:
+
[source,bash]
----
gcloud container clusters get-credentials <cluster_name> --region <region> --project <project>
----

This command generates a file containing the _kubeconfig_ in the location specified by the `KUBECONFIG` environment variable. By default, it will be `$HOME/.kube/config`.

* For unmanaged Azure, at the end of provisioning, the _kubeconfig_ is left in the workspace directory:

[source,bash]
----
ls ./.kube/config
./.kube/config
----
+
In turn, the alias "kw" may be used from the local container to interact with the cluster _worker_ (in EKS, the token used only lasts for 10 minutes):
+
[source,bash]
----
root@example-azure-control-plane:/# kw get nodes
NAME STATUS ROLES AGE VERSION
example-azure-control-plane-6kp94 Ready control-plane 60m v1.26.8
example-azure-control-plane-fgkcc Ready control-plane 63m v1.26.8
...
----

== Authentication in EKS

While not part of the _Stratio KEOS_ operation, it is essential to highlight how to enable https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html[authentication of other users in an EKS cluster] (the cluster creator user is authenticated by default).

To grant Kubernetes-admin permissions on the cluster, the user's ARN will be added to the _ConfigMap_ given below.

[source,bash]
----
$ kubectl -n kube-system edit cm aws-auth
..
data:
  mapUsers: |
    - groups:
      - system:masters
      userarn: <user_arn>
      username: kubernetes-admin
----

== Enable _Assume Role_ authorization in AWS for an EKS cluster

This procedure describes how to enable the _assume role_â€“based authorization method in AWS for an EKS cluster after it has been created.

=== Prerequisites

Before you begin, make sure of the following:

* You have created an IAM role with the required permissions (e.g. _stratio-assume-role_).
** Role type: _AWS Account_.
** Permissions: the same as those described in the EKS prerequisites.
* The role's _trust relationship_ is configured to allow the user who deployed the cluster to assume the role.
+
Example _trust relationship_:
+
[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:user/stratio-user"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
----

* The user is assigned a policy that includes the `sts:AssumeRole` action for the corresponding role.
+
Example policy:
+
[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "sts:AssumeRole",
      "Resource": "arn:aws:iam::123456789012:role/stratio-assume-role"
    }
  ]
}
----

NOTE: The minimum supported version of _cluster-operator_ for this method is 0.5.2.

=== Configure _assume role_ in the EKS cluster

NOTE: Replace the account, cluster name `eks-cl01`, and namespace `cluster-eks-cl01` with the values used in your environment.

. Create the `AWSClusterRoleIdentity` object.
+
[source,yaml]
----
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSClusterRoleIdentity
metadata:
  name: eks-cl01-role-identity
spec:
  roleARN: arn:aws:iam::123456789012:role/stratio-assume-role
  sessionName: eks-cl01-role-identity-session
  durationSeconds: 3600
  sourceIdentityRef:
    kind: AWSClusterControllerIdentity
    name: default
  allowedNamespaces:
    list:
    - cluster-eks-cl01
----

. Associate the `AWSClusterRoleIdentity` with the `AWSManagedControlPlane`.
+
[source,bash]
----
kubectl patch awsmanagedcontrolplane eks-cl01-control-plane \
  -n cluster-eks-cl01 \
  --type='merge' \
  -p '{"spec":{"identityRef":{"kind":"AWSClusterRoleIdentity","name":"eks-cl01-role-identity"}}}'
----

. Restart the `capa-controller-manager` deployment to apply the changes:
+
[source,bash]
----
kubectl rollout restart deployment capa-controller-manager -n capa-system
----

. Edit the `aws-auth` _ConfigMap_ adding the `iam_role`.
+
[source,yaml]
----
  mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: arn:aws:iam::123456789012:role/nodes.cluster-api-provider-aws.sigs.k8s.io
      username: system:node:{{EC2PrivateDNSName}}
    - groups:
      - capa-manager
      rolearn: arn:aws:iam::963353512345678901211234:role/stratio-assume-role
      username: stratio-assume-role
----

. Create the `ClusterRoleBinding` for the `capa-manager` group.
+
[source,yaml]
----
kubectl apply -f - <<EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: capa-manager-access
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: capa-manager-role
subjects:
- kind: Group
  name: capa-manager
  apiGroup: rbac.authorization.k8s.io
EOF
----

. Add permissions to the `ClusterRole` for the `capa-manager` group.
+
[source,bash]
----
kubectl patch clusterrole capa-manager-role \
  --type='json' \
  -p='[
    {"op": "add", "path": "/rules/-", "value": {"apiGroups":["apps"],"resources":["daemonsets"],"verbs":["get","list","watch","update"]}},
    {"op": "add", "path": "/rules/-", "value": {"apiGroups":[""],"resources":["pods"],"verbs":["get","list","watch"]}},
    {"op": "add", "path": "/rules/-", "value": {"apiGroups":[""],"resources":["nodes"],"verbs":["get","list","watch","patch"]}},
    {"op": "add", "path": "/rules/-", "value": {"apiGroups":[""],"resources":["pods/eviction"],"verbs":["create"]}}
  ]'
----

. Update the _cluster-operator_.
.. Update the _helmrelease_ associated with _cluster-operator_ to version 0.5.2 or later.
.. Update the corresponding _ConfigMap_ for that version.
... Modify the tag in `00-cluster-operator-helm-chart-default-values` to the desired version, e.g., 0.5.2.
.. Patch the _keoscluster-settings_ secret to include the `role_arn`.
+
[source,bash]
-----
kubectl -n kube-system patch secret keoscluster-settings \
  --type=json \
  -p='[{"op":"replace","path":"/data/credentials","value":"'$(kubectl -n kube-system get secret keoscluster-settings -o jsonpath="{.data.credentials}" | base64 -d | awk 'BEGIN{ORS="\n"} {print} END{print "role_arn = arn:aws:iam::123456789012:role/stratio-assume-role"}' | base64 -w0)'"}]'
-----

. Verify the configuration and permissions:
+
[source,bash]
----
# Check capi/capa logs
kubectl logs -f -n capa-system deployment/capa-controller-manager (or the name of the pod)
kubectl logs -f -n capi-system deployment/capi-controller-manager (or the name of the pod)

# Check cluster-operator logs
kubectl logs -f -n kube-system deployment/keoscluster-controller-manager (or the name of the pod)

# Check status and configuration
kubectl get awsclusterroleidentity
kubectl get awsmanagedcontrolplane -n cluster-eks-cl01
kubectl get configmap aws-auth -n kube-system -o yaml

# Verify role permissions
kubectl auth can-i get nodes --as=stratio-assume-role --as-group=capa-manager
kubectl auth can-i list nodes --as=stratio-assume-role --as-group=capa-manager
kubectl auth can-i list pods --as=stratio-assume-role --as-group=capa-manager
kubectl auth can-i update daemonsets --as=stratio-assume-role --as-group=capa-manager
----

[NOTE]
====
If the user performing the _assume role_ operation is updated, the following secrets must also be updated with the new credentials and, if applicable, the new role ARN:

* _capa-manager-bootstrap-credentials_ (for Capa).
* _keoscluster-settings_ (for the _cluster-operator_).
====

=== Cluster operations using _assume role_

Once _assume role_ is enabled, you can also manage the cluster from the command line by following these steps:

. *Check AWS CLI version*: make sure you have the latest version installed.
+
[source,bash]
----
aws --version
----

. *Export the AWS profile*:
+
[source,bash]
----
export AWS_PROFILE=<profile-name>
----

. *Assume the role and save credentials*.
+
[source,bash]
----
aws sts assume-role \
  --role-arn arn:aws:iam::<accountID>:role/<role-name> \
  --role-session-name eks-session > creds.json
----

. *Export the temporary credentials*.
+
[source,bash]
----
export AWS_ACCESS_KEY_ID=$(jq -r '.Credentials.AccessKeyId' creds.json)
export AWS_SECRET_ACCESS_KEY=$(jq -r '.Credentials.SecretAccessKey' creds.json)
export AWS_SESSION_TOKEN=$(jq -r '.Credentials.SessionToken' creds.json)
----

. *Update the _kubeconfig_*.
+
[source,bash]
----
aws eks update-kubeconfig --region <region> --name <cluster-name>
----

== Infrastructure operation

image::controllers.png[]

_Stratio KEOS_ enables multiple advanced operations to be performed by interacting with the _Stratio Cluster Operator_ (Infrastructure as Code, or IaC), which, in its reconciliation cycle, interacts with various providers to execute the requested operations.

=== Self-healing

image::self-healing.png[]

The cluster's self-healing capability is managed through the _MachineHealthCheck_ resource:

[source,bash]
----
$ kubectl -n cluster-example get mhc -o yaml
...
  spec:
    clusterName: example
    maxUnhealthy: 100%
    nodeStartupTimeout: 5m0s
    selector:
      matchLabels:
        keos.stratio.com/machine-role: example-worker-node
    unhealthyConditions:
    - status: Unknown
      timeout: 1m0s
      type: Ready
    - status: "False"
      timeout: 1m0s
      type: Ready
...
----

NOTE: In Unmanaged Azure, there will be a _MachineHealthCheck_ for the _control plane_ and another for the _worker_ nodes, whereas managed ones (EKS, GKE) will only have the latter.

==== Failover test on a node

In the event of a node failure, it will be detected by the controller, and the node will be replaced by deleting it and recreating another one of the same group, ensuring the same characteristics.

To simulate a VM failure, it will be deleted from the cloud provider's web console.

The recovery of the node comprises the following phases and estimated times (which may vary depending on the provider and the flavour):

[source,bash]
----
. Terminate VM from console: 0s
. New VM is Provisioning: 50s
. Old Machine is Deleted & the new one is Provisioned: 1m5s
. New Machine is Running & new k8s node is NotReady: 1m 50s
. New k8s node is Ready: 2m
----

=== Static scaling

Although manual scaling of an existing node group is discouraged, these operations are provided for cases where autoscaling is not available or for new node groups.

==== Scaling a _workers_ group

image::escalado-manual.png[]

To manually scale a group of _workers_, use the _KeosCluster_ resource:

[source,bash]
----
kubectl -n cluster-example-eks edit keoscluster
----

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - name: eks-prod-xlarge
      quantity: 9
      ...
----

Verify the change by checking the status of the _KeosCluster_ resource:

[source,bash]
----
kubectl -n cluster-<cluster_name> get keoscluster <cluster_name> --subresource=status
----

==== Create a new workers group

<<<<<<< HEAD
To create a new group of nodes, just create a new element to the array _worker++_++nodes_ of the _KeosCluster_ object:
=======
To create a new node group, add a new element to the _worker++_++nodes_ array in the _KeosCluster_ resource:
>>>>>>> ac219d93 ([PLT-1761][Clouds] Actualizar documentaciÃ³n 0.6.x (#811))

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - ...
    - name: eks-prod-xlarge
      quantity: 6
      max_size: 18
      min_size: 6
      size: m6i.xlarge
      labels:
        disktype: standard
      root_volume:
        size: 50
        type: gp3
        encrypted: true
      ssh_key: stg-key
----

Again, verify the change by checking the status of the _KeosCluster_ resource:

[source,bash]
----
kubectl -n cluster-<cluster_name> get keoscluster <cluster_name> --subresource=status
----

==== Vertical scaling

Vertical scaling of a node group is done by modifying the instance type in the _KeosCluster_ resource corresponding to the group.

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - name: eks-prod-xlarge
      size: m6i.2xlarge
      ...
----

=== Autoscaling

image::autoescalado.png[]

For node autoscaling, _cluster-autoscaler_ is used, which detects pods pending execution due to a lack of resources and scales groups of nodes according to the deployment filters.

This operation is performed in the API Server, with the controllers responsible for creating the VMs in the cloud provider and adding them to the cluster as Kubernetes _worker_ nodes.

Since autoscaling is based on the _cluster-autoscaler_, the minimum and maximum will be added to the node group in the _KeosCluster_ resource:

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - name: eks-prod-xlarge
      min_size: 6
      max_size: 21
      ...
----

==== Test

To test autoscaling, you can create a deployment with enough replicas to prevent them from running on the current nodes:

[source,bash]
----
kubectl create deploy test --replicas 1500 --image nginx:alpine
----

At the end of the test, remove the deployment:

[source,bash]
----
kubectl --kubeconfig demo-eks.kubeconfig delete deploy test
----

==== Logs

The logs of the _cluster-autoscaler_ can be viewed from its deployment:

[source,bash]
----
kubectl -n kube-system logs -f -l app.kubernetes.io/name=clusterapi-cluster-autoscaler
----

=== Kubernetes upgrade

The upgrade of the cluster to a higher version of Kubernetes will be performed in two parts within the same atomic process: first, the _control-plane_, and once this is on the new version, the worker nodes, iterating through each group and upgrading them one by one.

CAUTION: Upgrading the Kubernetes version of nodes in clusters where the image has not been specified may involve an OS upgrade.

image::upgrade-cp.png[]

image::upgrade-w.png[]

[CAUTION]
====
A misconfigured Pod Disruption Budget (PDB) can block the removal of a pod. This happens if the PDB requires at least one available replica, but the resource has only one deployed. In this case, the replica cannot be removed, preventing the node from draining, which may affect updates.

To avoid this issue:

. Please ensure that deployments have more than one replica if the PDB requires at least one replica to be available.
. Before updating the cluster, review this configuration to prevent blockages.
. If the resource has only one replica, you can temporarily remove the PDB to allow the update.
. Before upgrading the cluster, please verify the PDBs to avoid potential blocking issues.
. If a resource has only one replica, you can temporarily delete the PDB to allow the upgrade.

In EKS, for example, it is recommended to check if the `coredns` PDB exists in the `kube-system` namespace and delete it before upgrading the cluster:

[source,bash]
----
kubectl -n kube-system get poddisruptionbudget coredns
kubectl -n kube-system delete poddisruptionbudget coredns
----
====

==== Prerequisites

The version upgrade of a cluster in productive environments, and especially in unmanaged flavours, must be done with extreme caution. In particular, before upgrading, it is recommended to do a backup of the objects that manage the infrastructure with the following command:

[source,bash]
----
clusterctl --kubeconfig ./kubeconfig/path move -n cluster-<cluster_name> --to-directory ./backup/path/
----

In the case of a managed _control-plane_, it should be verified that the desired version of Kubernetes is supported by the provider.

===== EKS

Before upgrading EKS, ensure that the desired version is supported. To do this, you can use the following command:

[source,bash]
----
aws eks describe-addon-versions | jq -r ".addons[] | .addonVersions[] | .compatibilities[] | .clusterVersion" | sort -nr | uniq | head -4
----

===== Azure unmanaged

The _GlobalNetworkPolicy_ created for the _control-plane_ in the _Stratio KEOS_ installation phase should be modified so that it *permits all node networking momentarily* while the version upgrade is running.

Once completed, the internal IPs of the nodes and the tunnel IPs assigned to those nodes should be updated:

[source,bash]
----
kubectl get nodes -l node-role.kubernetes.io/control-plane= -ojson | jq -r '.items[].status.addresses[] | select(.type=="InternalIP").address + "\/32"'
----

[source,bash]
----
IPAMHANDLERS=$(kw get ipamhandles -oname | grep control-plane)
for handler in $IPAMHANDLERS; do kw get $handler -o json | jq -r '.spec.block | keys[]' | sed 's/\/.*/\/32/'; done
----

==== Initiate the upgrade

<<<<<<< HEAD
To initiate the upgrade, once the prerequisites are satisfied, a patch of _spec.k8s++_++version_ will be run on the _KeosCluster_ object:
=======
To initiate the upgrade, once the prerequisites are met, a patch to _spec.k8s++_++version_ will be applied to the _KeosCluster_ resource:
>>>>>>> ac219d93 ([PLT-1761][Clouds] Actualizar documentaciÃ³n 0.6.x (#811))

[source,bash]
----
kubectl -n cluster-<cluster_name> patch KeosCluster <cluster_name> --type merge -p '{"spec": {"k8s_version": "v1.28.1"}}'
----

NOTE: The controller provisions a new node from the _workers_ cluster with the updated version and, once it is _Ready_ in Kubernetes, removes a node with the old version. In this way, it always ensures the configured number of nodes.

==== Checking etcd

One way to ensure that etcd is correct after updating an unmanaged _control-plane_ is to open a terminal on any pod of etcd, view the cluster status, and compare the IPs of the registered members with those of the _control-plane_ nodes.

[source,bash]
----
k -n kube-system exec -ti etcd-<control-plane-node> sh

alias e="etcdctl --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt "
e endpoint status
e endpoint status -w table --cluster
e member list
e member remove <member-id>
----

=== Cluster removal

[NOTE]
.Preliminary considerations
====
Before deleting the cloud provider resources generated by _Stratio Cloud Provisioner_, you must delete those that the _keos-installer_ has created or any external automation (for example, the _Services_ of type _LoadBalancer_).

Also, note that the process requires the _clusterctl_ binary on the bastion machine (any computer with access to the API Server) on which it will run.
====

Run the following steps to perform the cluster removal:

. Create a local cluster indicating that no resources should be created in the cloud provider.
+
[source,bash]
----
[bastion]$ sudo ./bin/cloud-provisioner create cluster --name <cluster_name> --descriptor cluster.yaml --vault-password <my-passphrase> --avoid-creation
----

. Pause the controller of the _Stratio Cluster Operator_:
+
[source,bash]
----
[bastion]$ kubectl --kubeconfig $KUBECONFIG -n kube-system scale deployment keoscluster-controller-manager --replicas 0
----

. Move the cluster _worker_ management to the local cluster using the corresponding _kubeconfig_ (note that for managed _control-planes_, the provider's _kubeconfig_ will be needed). To ensure this step, look for the following text in the command output: "Moving Cluster API objects Clusters=1".
+
[source,bash]
----
[bastion]$ sudo clusterctl --kubeconfig $KUBECONFIG move -n cluster-<cluster_name> --to-kubeconfig /root/.kube/config
----

. Access the local cluster and delete the cluster _worker_.
+
[source,bash]
----
[bastion]$ sudo docker exec -ti <nombre_cluster>-control-plane bash
root@<nombre_del_cluster>-control-plane:/# kubectl -n cluster-<nombre_del_cluster> delete cl --all
cluster.cluster.x-k8s.io "<nombre_del_cluster>" eliminado
root@<nombre_del_cluster>-plano-de-control:/#
----

. Finally, remove the local cluster.
+
[source,bash]
----
[bastion]$ sudo ./bin/cloud-provisioner delete cluster --name <nombre_cluster>
----

== Offline installation

To learn how to perform an installation where the workload images of the cluster come from repositories accessible from environments without internet access, see the xref:operations-manual:offline-installation.adoc[offline installation manual].

== Credential management

To manage the credentials configured in the cluster, refer to the xref:operations-manual:credentials.adoc[credential management documentation].
